{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3b855a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Internshala scraping...\n",
      "Scraping page 1...\n",
      "Found 50 internships using selector: div.internship_meta\n",
      "Processing internship 1/50\n",
      "Successfully extracted: Graphic Design at zFrames Media\n",
      "Processing internship 2/50\n",
      "Successfully extracted: Brand Ambassador â€“ On-Camera Content Creator (Wellness + Lifestyle) at Herbnexus Pvt Ltd\n",
      "Processing internship 3/50\n",
      "Successfully extracted: Human Resources (HR) at Lime Learn Eduserv Private Limited\n",
      "Processing internship 4/50\n",
      "Successfully extracted: Data Entry at SANNA CAPITAL PRIVATE LIMITED\n",
      "Processing internship 5/50\n",
      "Successfully extracted: Data Entry at Cherag Traders\n",
      "Processing internship 6/50\n",
      "Successfully extracted: Email Marketing at Web3Task\n",
      "Processing internship 7/50\n",
      "Successfully extracted: Game Tester at Renske Technologies Inc\n",
      "Processing internship 8/50\n",
      "Successfully extracted: Video Editing/Making at zFrames Media\n",
      "Processing internship 9/50\n",
      "Successfully extracted: Public Speaking & Theatre Coach at Utsaah Learning Private Limited\n",
      "Processing internship 10/50\n",
      "Successfully extracted: Media & Public Relations (PR) at Simran Khatri\n",
      "Processing internship 11/50\n",
      "Successfully extracted: Marketing at Times Internet\n",
      "Processing internship 12/50\n",
      "Successfully extracted: Video Editing/Making at 603 The CoWorking Space India\n",
      "Processing internship 13/50\n",
      "Successfully extracted: Content and Social Media Marketing at Lakme Lever Private Limited\n",
      "Processing internship 14/50\n",
      "Successfully extracted: Data Entry at Teamlease Services Limited\n",
      "Processing internship 15/50\n",
      "Successfully extracted: Business Research at BrandWagon - The Financial Express\n",
      "Processing internship 16/50\n",
      "Successfully extracted: Human Resources (HR) at Godrej Infotech\n",
      "Processing internship 17/50\n",
      "Successfully extracted: Video Editing/Making at Amnet Systems Private Limited\n",
      "Processing internship 18/50\n",
      "Successfully extracted: Content and Social Media Marketing at Blue Tokai Coffee Roasters\n",
      "Processing internship 19/50\n",
      "Successfully extracted: Business Research at Financial Express (Indian Express Group)\n",
      "Processing internship 20/50\n",
      "Successfully extracted: Business Development (Sales) at Business Development (Sales)\n",
      "Processing internship 21/50\n",
      "Successfully extracted: Financial Stock Market Research at Focus Wise Solution\n",
      "Processing internship 22/50\n",
      "Successfully extracted: Finance & Accounting at Air India SATS Airport Services Private Limited\n",
      "Processing internship 23/50\n",
      "Successfully extracted: Video Production at NoBroker\n",
      "Processing internship 24/50\n",
      "Successfully extracted: Search Engine Optimization (SEO) at NoBroker\n",
      "Processing internship 25/50\n",
      "Successfully extracted: Sales and Marketing at Wheebox\n",
      "Processing internship 26/50\n",
      "Successfully extracted: Graphic Design at Petpooja\n",
      "Processing internship 27/50\n",
      "Successfully extracted: Business Development (Sales) at Business Development (Sales)\n",
      "Processing internship 28/50\n",
      "Successfully extracted: Sales Consultant at Aditya Birla Fashion & Retail Limited (ABFRL)\n",
      "Processing internship 29/50\n",
      "Successfully extracted: Textile Designer at Group Bayport\n",
      "Processing internship 30/50\n",
      "Successfully extracted: Stock Market Learning & Outreach at Focus Wise Solution\n",
      "Processing internship 31/50\n",
      "Successfully extracted: Recruitment at Kotak Mahindra Bank Limited\n",
      "Processing internship 32/50\n",
      "Successfully extracted: Visual Design at More Retail Private Limited\n",
      "Processing internship 33/50\n",
      "Successfully extracted: E-Commerce Operations at Purplle\n",
      "Processing internship 34/50\n",
      "Successfully extracted: Human Resources (HR) at Focus Wise Solution\n",
      "Processing internship 35/50\n",
      "Successfully extracted: Mobile App Development at Thomas Cook\n",
      "Processing internship 36/50\n",
      "Successfully extracted: Finance & Accounting at Thomas Cook\n",
      "Processing internship 37/50\n",
      "Successfully extracted: Sampling at Navneet Education Limited\n",
      "Processing internship 38/50\n",
      "Successfully extracted: Business Development (Sales) at Business Development (Sales)\n",
      "Processing internship 39/50\n",
      "Successfully extracted: Graphics Designer at Sri Sri Tattva\n",
      "Processing internship 40/50\n",
      "Successfully extracted: Content and Social Media Marketing at Anarock Property Consultant Private Limited\n",
      "Processing internship 41/50\n",
      "Successfully extracted: Graphic Design at Shemaroo Entertainment Limited\n",
      "Processing internship 42/50\n",
      "Successfully extracted: Google My Business (GMB) at Yum! Brands\n",
      "Processing internship 43/50\n",
      "Successfully extracted: Digital Marketing at GR Brand\n",
      "Processing internship 44/50\n",
      "Successfully extracted: Human Resources (HR) Manager at Protium\n",
      "Processing internship 45/50\n",
      "Successfully extracted: Marketing and Sales at Reliance Nippon Life Insurance\n",
      "Processing internship 46/50\n",
      "Successfully extracted: Human Resources (HR) at CollegeDekho.com\n",
      "Processing internship 47/50\n",
      "Successfully extracted: Logistics And Operations at Indian Gifts Portal\n",
      "Processing internship 48/50\n",
      "Successfully extracted: Sales and Marketing Manager at Imarticus Learning\n",
      "Processing internship 49/50\n",
      "Successfully extracted: Content Writing at Marpu Foundation\n",
      "Processing internship 50/50\n",
      "Successfully extracted: Business Development (Sales) at Fern Hotels & Resorts\n",
      "Page 1 complete. Total internships so far: 50\n",
      "Scraping page 2...\n",
      "Found 40 internships using selector: div.internship_meta\n",
      "Processing internship 1/40\n",
      "Successfully extracted: UI/UX Design at Thomas Cook\n",
      "Processing internship 2/40\n",
      "Successfully extracted: Sales and Marketing at Reliance Nippon Life Insurance\n",
      "Processing internship 3/40\n",
      "Successfully extracted: Video Editing/Making at LetsTransport\n",
      "Processing internship 4/40\n",
      "Successfully extracted: Chartered Accountancy (CA) at JM Financial\n",
      "Processing internship 5/40\n",
      "Successfully extracted: Operations at Decathlon Sport India Private Limited\n",
      "Processing internship 6/40\n",
      "Successfully extracted: Motion Graphics at More Retail Private Limited\n",
      "Processing internship 7/40\n",
      "Successfully extracted: Content Writing at NoBroker\n",
      "Processing internship 8/40\n",
      "Successfully extracted: Marketing Automation Executive at Thomas Cook\n",
      "Processing internship 9/40\n",
      "Successfully extracted: Content and Social Media Marketing at Puja Gupta\n",
      "Processing internship 10/40\n",
      "Successfully extracted: Computational Physics/Chemistry/Materials Science at Bhabha Atomic Research Centre (BARC)\n",
      "Processing internship 11/40\n",
      "Successfully extracted: Law/Legal at MoveInSync Technology Solutions Private Limited\n",
      "Processing internship 12/40\n",
      "Successfully extracted: Law/Legal at Europ Assistance\n",
      "Processing internship 13/40\n",
      "Successfully extracted: Front End Development at Ghar Pe Shiksha\n",
      "Processing internship 14/40\n",
      "Successfully extracted: Digital Media Intern For Video Production at Aster DM Healthcare\n",
      "Processing internship 15/40\n",
      "Successfully extracted: Business Development (Sales) at Confederation Of Indian Industry (CII)\n",
      "Processing internship 16/40\n",
      "Successfully extracted: Sales Apprentice at Halma\n",
      "Processing internship 17/40\n",
      "Successfully extracted: Law/Legal at S Singh\n",
      "Processing internship 18/40\n",
      "Successfully extracted: Product Design at Scaler\n",
      "Processing internship 19/40\n",
      "Successfully extracted: B2B Business Development at Scaler\n",
      "Processing internship 20/40\n",
      "Successfully extracted: Recruitment at JM Financial\n",
      "Processing internship 21/40\n",
      "Successfully extracted: Creative Design at Waaree Energies Limited\n",
      "Processing internship 22/40\n",
      "Successfully extracted: Digital Ad Sales at Times Internet\n",
      "Processing internship 23/40\n",
      "Successfully extracted: Hotel Management(Chef) at Theobroma Foods Private Limited\n",
      "Processing internship 24/40\n",
      "Successfully extracted: Company Secretary (CS) at Company Secretary (CS)\n",
      "Processing internship 25/40\n",
      "Successfully extracted: Business Development (Sales) at ILux Electricals\n",
      "Processing internship 26/40\n",
      "Successfully extracted: Legal Research at CollegeDekho.com\n",
      "Processing internship 27/40\n",
      "Successfully extracted: Inside Sales at Unacademy (Sorting Hat Technologies Private Limited)\n",
      "Processing internship 28/40\n",
      "Successfully extracted: Sales at Halma\n",
      "Processing internship 29/40\n",
      "Successfully extracted: Human Resources (HR) at Audi Mumbai West\n",
      "Processing internship 30/40\n",
      "Successfully extracted: Video Editing/Making at Zeta India\n",
      "Processing internship 31/40\n",
      "Successfully extracted: Finance at IREED Academy India Private Limited\n",
      "Processing internship 32/40\n",
      "Successfully extracted: Compute Platform Engineer at Corteva Agriscience\n",
      "Processing internship 33/40\n",
      "Successfully extracted: Corporate Booking Executive at Mahindra Logistics Limited\n",
      "Processing internship 34/40\n",
      "Successfully extracted: HR Information Systems at Mahindra Logistics Limited\n",
      "Processing internship 35/40\n",
      "Successfully extracted: Operations Executive - B2C at Mahindra Logistics Limited\n",
      "Processing internship 36/40\n",
      "Successfully extracted: Operations Executive - Central Initiatives at Mahindra Logistics Limited\n",
      "Processing internship 37/40\n",
      "Successfully extracted: Sales and Marketing at Cook N Klean\n",
      "Processing internship 38/40\n",
      "Successfully extracted: Inside Sales at Unacademy (Sorting Hat Technologies Private Limited)\n",
      "Processing internship 39/40\n",
      "Successfully extracted: Sales at Aditya Birla Fashion & Retail Limited (ABFRL)\n",
      "Processing internship 40/40\n",
      "Successfully extracted: Corporate Sales at Reckitt\n",
      "Page 2 complete. Total internships so far: 90\n",
      "Scraping page 3...\n",
      "Error scraping page 3: HTTPSConnectionPool(host='internshala.com', port=443): Read timed out.\n",
      "Scraping page 4...\n",
      "Found 40 internships using selector: div.internship_meta\n",
      "Processing internship 1/40\n",
      "Successfully extracted: Graphic Design at Revogreen Technologies Private Limited\n",
      "Processing internship 2/40\n",
      "Successfully extracted: UX/UI Design at Brandshark\n",
      "Processing internship 3/40\n",
      "Successfully extracted: Human Resources (HR) at ADORE\n",
      "Processing internship 4/40\n",
      "Successfully extracted: Operations at Growify Digital\n",
      "Processing internship 5/40\n",
      "Successfully extracted: Accounting & Bookkeeping at Morphdhome Solpro Private Limited\n",
      "Processing internship 6/40\n",
      "Successfully extracted: Human Resources (HR) at Universal Tribes\n",
      "Processing internship 7/40\n",
      "Successfully extracted: Product Marketing at Elythra Edufyi Tech Solutions\n",
      "Processing internship 8/40\n",
      "Successfully extracted: Operations Management at Pawzz\n",
      "Processing internship 9/40\n",
      "Successfully extracted: Graphic Design at Eccentric Engine\n",
      "Processing internship 10/40\n",
      "Successfully extracted: Inside Sales at Teach Maven\n",
      "Processing internship 11/40\n",
      "Successfully extracted: Business Development (Sales) at 4 Way Technologies\n",
      "Processing internship 12/40\n",
      "Successfully extracted: Telecalling at Magix-Engage Private Limited\n",
      "Processing internship 13/40\n",
      "Successfully extracted: GTM Strategy & City Operations at FatafatGo Technologies Private Limited\n",
      "Processing internship 14/40\n",
      "Successfully extracted: Capital Modelling at RSA Actuarial Services India Private Limited\n",
      "Processing internship 15/40\n",
      "Successfully extracted: Law/Legal at Gun Rajan Constructions\n",
      "Processing internship 16/40\n",
      "Successfully extracted: MERN Stack Developer at Leistung Technologies\n",
      "Processing internship 17/40\n",
      "Successfully extracted: Digital Marketing at 72Cubes\n",
      "Processing internship 18/40\n",
      "Successfully extracted: Architecture at Knowledge Center, Gurgaon\n",
      "Processing internship 19/40\n",
      "Successfully extracted: Law/Legal at Tax-O-Smart LLP\n",
      "Processing internship 20/40\n",
      "Successfully extracted: Influencer Marketing at Brandshark\n",
      "Processing internship 21/40\n",
      "Successfully extracted: Recruitment at SuperHuman Race Private Limited\n",
      "Processing internship 22/40\n",
      "Successfully extracted: Business Development (Sales) (Male) at Tyresnmore.com\n",
      "Processing internship 23/40\n",
      "Successfully extracted: Business Development (Sales) at Tech Analogy\n",
      "Processing internship 24/40\n",
      "Successfully extracted: Digital Marketing at Eduminatti\n",
      "Processing internship 25/40\n",
      "Successfully extracted: Digital Marketing at METGUARD\n",
      "Processing internship 26/40\n",
      "Successfully extracted: Community Management at Groovly\n",
      "Processing internship 27/40\n",
      "Successfully extracted: Creative at Dashtoon\n",
      "Processing internship 28/40\n",
      "Successfully extracted: Graphic Design at FiftySeven\n",
      "Processing internship 29/40\n",
      "Successfully extracted: Online Marketing at Groovly\n",
      "Processing internship 30/40\n",
      "Successfully extracted: Content Writing at CoreSwipe Technologies (OPC) Private Limited\n",
      "Processing internship 31/40\n",
      "Successfully extracted: Search Engine Optimization (SEO) at Deedok It Solutions\n",
      "Processing internship 32/40\n",
      "Successfully extracted: Digital Marketing at Xsoln\n",
      "Processing internship 33/40\n",
      "Successfully extracted: Partner Onboarding (Non-Technical Role) at Inspacco\n",
      "Processing internship 34/40\n",
      "Successfully extracted: Search Engine Optimization (SEO) at Search Digitally\n",
      "Processing internship 35/40\n",
      "Successfully extracted: Administration at Bharti Rakheja\n",
      "Processing internship 36/40\n",
      "Successfully extracted: Business Development (Sales) at Zell Education\n",
      "Processing internship 37/40\n",
      "Successfully extracted: Search Engine Optimization (SEO) at BA Solutions India Private Limited\n",
      "Processing internship 38/40\n",
      "Successfully extracted: Field Sales at Kriips Technologies Private Limited\n",
      "Processing internship 39/40\n",
      "Successfully extracted: AI Content Creation at Be Rolling Media\n",
      "Processing internship 40/40\n",
      "Successfully extracted: Video Editing/Making at Zero Dimensions\n",
      "Page 4 complete. Total internships so far: 130\n",
      "Scraping page 5...\n",
      "Found 40 internships using selector: div.internship_meta\n",
      "Processing internship 1/40\n",
      "Successfully extracted: Performance Ad Writing at Ayurveda House Private Limited\n",
      "Processing internship 2/40\n",
      "Successfully extracted: AI Tools Research â€“ Quantitative Trading & Automation at QuantGlobal\n",
      "Processing internship 3/40\n",
      "Successfully extracted: Campaign Performance Specialist at Enout\n",
      "Processing internship 4/40\n",
      "Successfully extracted: Partner Support & Calling (Telecalling Profile) at Inspacco\n",
      "Processing internship 5/40\n",
      "Successfully extracted: Graphic Design at GigFloww\n",
      "Processing internship 6/40\n",
      "Successfully extracted: Graphic Design at Nutri Couture\n",
      "Processing internship 7/40\n",
      "Successfully extracted: Meta Ad Writing at Ayurveda House Private Limited\n",
      "Processing internship 8/40\n",
      "Successfully extracted: UI/UX Design at Sugary\n",
      "Processing internship 9/40\n",
      "Successfully extracted: 3D Animator at QuadB Tech\n",
      "Processing internship 10/40\n",
      "Successfully extracted: Founders Office at Zi Cloud\n",
      "Processing internship 11/40\n",
      "Successfully extracted: Talent Acquisition at Sugary\n",
      "Processing internship 12/40\n",
      "Successfully extracted: Business Operation (Outbound Process) at Ketto Online Ventures Private Limited\n",
      "Processing internship 13/40\n",
      "Successfully extracted: Video Editing/Making at Skil.ai\n",
      "Processing internship 14/40\n",
      "Successfully extracted: Sales and Marketing at Amaya Decors\n",
      "Processing internship 15/40\n",
      "Successfully extracted: Sales and Marketing at Sales and Marketing\n",
      "Processing internship 16/40\n",
      "Successfully extracted: Digital Marketing at YOURS FAITHFULLY ADVISORS LLP\n",
      "Processing internship 17/40\n",
      "Successfully extracted: Civil & Mechanical Draftsmen at Morphdhome Solpro Private Limited\n",
      "Processing internship 18/40\n",
      "Successfully extracted: Styling at The Souled Store\n",
      "Processing internship 19/40\n",
      "Successfully extracted: Copywriting at Collective Artists Network\n",
      "Processing internship 20/40\n",
      "Successfully extracted: Data Science at Aadhvik Technologies\n",
      "Processing internship 21/40\n",
      "Successfully extracted: Growth Hacker at InstaWeb Labs Private Limited\n",
      "Processing internship 22/40\n",
      "Successfully extracted: Business Development at Emversity\n",
      "Processing internship 23/40\n",
      "Successfully extracted: Product Curation And Sales Trainee at Avaesa\n",
      "Processing internship 24/40\n",
      "Successfully extracted: Cyber Security at Aadhvik Technologies\n",
      "Processing internship 25/40\n",
      "Successfully extracted: Marketing Agent at Guru Global Pre School\n",
      "Processing internship 26/40\n",
      "Successfully extracted: Business Development (Sales) at Natturz Bio Kontrol Private Limited\n",
      "Processing internship 27/40\n",
      "Successfully extracted: Empowering Social Entrepreneurs at Basti Ki Pathshala Foundation\n",
      "Processing internship 28/40\n",
      "Successfully extracted: Creative Design at Mindwatt Industries\n",
      "Processing internship 29/40\n",
      "Successfully extracted: Event Management at Blue Sparrow Science Party\n",
      "Processing internship 30/40\n",
      "Successfully extracted: Inside Sales at Isourse\n",
      "Processing internship 31/40\n",
      "Successfully extracted: Psychology Research & Content Development at Mylifekompass\n",
      "Processing internship 32/40\n",
      "Successfully extracted: Community Influencing at Basti Ki Pathshala Foundation\n",
      "Processing internship 33/40\n",
      "Successfully extracted: Telecalling(Female) at Elevate X Solutions\n",
      "Processing internship 34/40\n",
      "Successfully extracted: Content and Social Media Marketing at Indogenmed Healthacare Private Ltd.\n",
      "Processing internship 35/40\n",
      "Successfully extracted: Digital Marketing at Brandshark\n",
      "Processing internship 36/40\n",
      "Successfully extracted: Content and Social Media Marketing at Akshita Kalra\n",
      "Processing internship 37/40\n",
      "Successfully extracted: Digital Marketing at Eduminatti\n",
      "Processing internship 38/40\n",
      "Successfully extracted: Customer Service/Customer Support at NestAway Technologies Private Limited\n",
      "Processing internship 39/40\n",
      "Successfully extracted: Customer Service/Customer Support at Prisms\n",
      "Processing internship 40/40\n",
      "Successfully extracted: Civil Engineering at SHRI MISRI GROUP\n",
      "Page 5 complete. Total internships so far: 170\n",
      "Scraped 170 internships from Internshala\n",
      "Data saved to internship_market_data.csv\n",
      "Total internships scraped: 168\n",
      "\n",
      "=== SCRAPING SUMMARY ===\n",
      "Total Internships: 168\n",
      "Paid Internships: 163\n",
      "Unpaid Internships: 5\n",
      "Average Stipend (Paid): 12051.273006134968\n",
      "Top Domains: {'Other': 52, 'Marketing': 36, 'Design': 23, 'Sales': 22, 'HR': 9}\n",
      "Location Types: {'Onsite': 149, 'Remote': 19}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import re\n",
    "from datetime import datetime\n",
    "import json\n",
    "from urllib.parse import urljoin, urlparse\n",
    "import random\n",
    "\n",
    "class InternshipScraper:\n",
    "    def __init__(self):\n",
    "        self.headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
    "            'Accept-Language': 'en-US,en;q=0.5',\n",
    "            'Accept-Encoding': 'gzip, deflate',\n",
    "            'Connection': 'keep-alive',\n",
    "            'Upgrade-Insecure-Requests': '1'\n",
    "        }\n",
    "        self.session = requests.Session()\n",
    "        self.session.headers.update(self.headers)\n",
    "        self.internships_data = []\n",
    "        \n",
    "    def extract_stipend_value(self, stipend_text):\n",
    "        \"\"\"Extract numeric stipend value from text\"\"\"\n",
    "        if not stipend_text or 'unpaid' in stipend_text.lower():\n",
    "            return 0\n",
    "        \n",
    "        numbers = re.findall(r'[\\d,]+', stipend_text.replace(',', ''))\n",
    "        if numbers:\n",
    "            \n",
    "            nums = [int(n) for n in numbers]\n",
    "            return sum(nums) / len(nums)\n",
    "        return 0\n",
    "    \n",
    "    def categorize_domain(self, title, skills=\"\"):\n",
    "        \"\"\"Categorize internship into domain based on title and skills\"\"\"\n",
    "        title_lower = title.lower()\n",
    "        skills_lower = skills.lower() if skills else \"\"\n",
    "        combined = f\"{title_lower} {skills_lower}\"\n",
    "        \n",
    "        categories = {\n",
    "            'Tech': ['software', 'developer', 'programming', 'python', 'java', 'react', 'node', 'full stack', 'backend', 'frontend', 'web dev', 'app dev', 'data science', 'machine learning', 'ai', 'ml'],\n",
    "            'Design': ['ui', 'ux', 'graphic', 'design', 'figma', 'photoshop', 'illustrator', 'canva', 'creative'],\n",
    "            'Marketing': ['marketing', 'digital marketing', 'seo', 'social media', 'content', 'advertising', 'brand', 'campaign'],\n",
    "            'Sales': ['sales', 'business development', 'bd', 'lead generation', 'customer acquisition'],\n",
    "            'Finance': ['finance', 'accounting', 'financial', 'investment', 'banking', 'audit'],\n",
    "            'Operations': ['operations', 'logistics', 'supply chain', 'project management', 'admin'],\n",
    "            'HR': ['human resource', 'hr', 'recruitment', 'talent acquisition', 'people'],\n",
    "            'Content': ['content writing', 'copywriting', 'blog', 'writer', 'journalism', 'editor'],\n",
    "            'Research': ['research', 'analyst', 'market research', 'data analyst', 'business analyst']\n",
    "        }\n",
    "        \n",
    "        for category, keywords in categories.items():\n",
    "            if any(keyword in combined for keyword in keywords):\n",
    "                return category\n",
    "        \n",
    "        return 'Other'\n",
    "    \n",
    "    def scrape_internshala(self, max_pages=5):\n",
    "        \"\"\"Scrape internships from Internshala\"\"\"\n",
    "        print(\"Starting Internshala scraping...\")\n",
    "        base_url = \"https://internshala.com/internships\"\n",
    "        \n",
    "        for page in range(1, max_pages + 1):\n",
    "            try:\n",
    "                print(f\"Scraping page {page}...\")\n",
    "                url = f\"{base_url}/page-{page}\" if page > 1 else base_url\n",
    "                \n",
    "                response = self.session.get(url, timeout=10)\n",
    "                response.raise_for_status()\n",
    "                \n",
    "                soup = BeautifulSoup(response.content, 'html.parser')\n",
    "                \n",
    "               \n",
    "                internship_containers = []\n",
    "                \n",
    "                \n",
    "                selectors_to_try = [\n",
    "                    'div.internship_meta',\n",
    "                    'div[class*=\"internship\"]',\n",
    "                    'div.individual_internship',\n",
    "                    'div.internship-item',\n",
    "                    'div[data-internship-id]',\n",
    "                    'div.container-fluid.individual_internship',\n",
    "                    'div.view_detail_button'\n",
    "                ]\n",
    "                \n",
    "                for selector in selectors_to_try:\n",
    "                    try:\n",
    "                        containers = soup.select(selector)\n",
    "                        if containers:\n",
    "                            internship_containers = containers\n",
    "                            print(f\"Found {len(containers)} internships using selector: {selector}\")\n",
    "                            break\n",
    "                    except:\n",
    "                        continue\n",
    "                \n",
    "                \n",
    "                if not internship_containers:\n",
    "                    all_divs = soup.find_all('div')\n",
    "                    for div in all_divs:\n",
    "                        if div.get_text() and any(keyword in div.get_text().lower() for keyword in ['internship', 'stipend', 'apply by']):\n",
    "                            text = div.get_text().lower()\n",
    "                            if 'stipend' in text and ('month' in text or 'week' in text):\n",
    "                                internship_containers.append(div)\n",
    "                    \n",
    "                   \n",
    "                    internship_containers = [div for div in internship_containers if len(div.get_text()) > 100][:20]\n",
    "                    print(f\"Found {len(internship_containers)} internships using broad search\")\n",
    "                \n",
    "                if not internship_containers:\n",
    "                    print(f\"No internships found on page {page}\")\n",
    "                    with open(f'debug_page_{page}.html', 'w', encoding='utf-8') as f:\n",
    "                        f.write(soup.prettify())\n",
    "                    print(f\"Saved HTML to debug_page_{page}.html for inspection\")\n",
    "                    break\n",
    "                \n",
    "                for i, container in enumerate(internship_containers):\n",
    "                    try:\n",
    "                        print(f\"Processing internship {i+1}/{len(internship_containers)}\")\n",
    "                        internship_data = self.extract_internshala_data(container)\n",
    "                        if internship_data:\n",
    "                            self.internships_data.append(internship_data)\n",
    "                            print(f\"Successfully extracted: {internship_data['job_title']} at {internship_data['company_name']}\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error extracting internship data {i+1}: {e}\")\n",
    "                        continue\n",
    "                \n",
    "                print(f\"Page {page} complete. Total internships so far: {len(self.internships_data)}\")\n",
    "                \n",
    "                time.sleep(random.uniform(2, 4))\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error scraping page {page}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        print(f\"Scraped {len(self.internships_data)} internships from Internshala\")\n",
    "    \n",
    "    def extract_internshala_data(self, container):\n",
    "        \"\"\"Extract data from individual internship container\"\"\"\n",
    "        try:\n",
    "            container_text = container.get_text()\n",
    "            company_name = \"Not specified\"\n",
    "            company_selectors = [\n",
    "                'a.link_display_like_text',\n",
    "                'a[href*=\"company\"]',\n",
    "                'h4 a',\n",
    "                'h5 a',\n",
    "                '.company-name'\n",
    "            ]\n",
    "            \n",
    "            for selector in company_selectors:\n",
    "                company_elem = container.select_one(selector)\n",
    "                if company_elem:\n",
    "                    company_name = company_elem.get_text(strip=True)\n",
    "                    break\n",
    "            \n",
    "            if company_name == \"Not specified\":\n",
    "                company_match = re.search(r'Company[:\\s]*([^\\n]+)', container_text, re.IGNORECASE)\n",
    "                if company_match:\n",
    "                    company_name = company_match.group(1).strip()\n",
    "            \n",
    "            job_title = \"Not specified\"\n",
    "            title_selectors = [\n",
    "                'h3.heading_4_5',\n",
    "                'h4.heading_4_5',\n",
    "                'h3',\n",
    "                'h4',\n",
    "                '.job-title',\n",
    "                '.internship-title'\n",
    "            ]\n",
    "            \n",
    "            for selector in title_selectors:\n",
    "                title_elem = container.select_one(selector)\n",
    "                if title_elem:\n",
    "                    job_title = title_elem.get_text(strip=True)\n",
    "                    break\n",
    "            \n",
    "            if job_title == \"Not specified\":\n",
    "                lines = container_text.split('\\n')\n",
    "                for line in lines:\n",
    "                    if 'intern' in line.lower() and len(line.strip()) < 100:\n",
    "                        job_title = line.strip()\n",
    "                        break\n",
    "            \n",
    "            location = \"Not specified\"\n",
    "            location_selectors = [\n",
    "                'a[href*=\"location\"]',\n",
    "                '.location',\n",
    "                '.internship-location'\n",
    "            ]\n",
    "            \n",
    "            for selector in location_selectors:\n",
    "                location_elem = container.select_one(selector)\n",
    "                if location_elem:\n",
    "                    location = location_elem.get_text(strip=True)\n",
    "                    break\n",
    "            \n",
    "            if location == \"Not specified\":\n",
    "                location_patterns = [\n",
    "                    r'Location[:\\s]*([^\\n]+)',\n",
    "                    r'Work from home|Remote',\n",
    "                    r'([A-Z][a-z]+(?:\\s+[A-Z][a-z]+)*),?\\s*(?:India|IN)?'\n",
    "                ]\n",
    "                for pattern in location_patterns:\n",
    "                    location_match = re.search(pattern, container_text, re.IGNORECASE)\n",
    "                    if location_match:\n",
    "                        location = location_match.group(1).strip() if location_match.groups() else location_match.group(0)\n",
    "                        break\n",
    "        \n",
    "            stipend_raw = \"Not specified\"\n",
    "            stipend_selectors = [\n",
    "                'span.stipend',\n",
    "                '.stipend',\n",
    "                '.salary',\n",
    "                '.compensation'\n",
    "            ]\n",
    "            \n",
    "            for selector in stipend_selectors:\n",
    "                stipend_elem = container.select_one(selector)\n",
    "                if stipend_elem:\n",
    "                    stipend_raw = stipend_elem.get_text(strip=True)\n",
    "                    break\n",
    "            \n",
    "            if stipend_raw == \"Not specified\":\n",
    "                stipend_patterns = [\n",
    "                    r'â‚¹\\s*[\\d,]+(?:\\s*-\\s*â‚¹?\\s*[\\d,]+)?',\n",
    "                    r'Stipend[:\\s]*([^\\n]+)',\n",
    "                    r'(\\d+(?:,\\d+)*)\\s*(?:per month|/month)',\n",
    "                    r'Unpaid',\n",
    "                    r'Performance based'\n",
    "                ]\n",
    "                for pattern in stipend_patterns:\n",
    "                    stipend_match = re.search(pattern, container_text, re.IGNORECASE)\n",
    "                    if stipend_match:\n",
    "                        stipend_raw = stipend_match.group(0)\n",
    "                        break\n",
    "            \n",
    "            stipend_numeric = self.extract_stipend_value(stipend_raw)\n",
    "            \n",
    "            duration_raw = \"Not specified\"\n",
    "            duration_patterns = [\n",
    "                r'(\\d+)\\s*months?',\n",
    "                r'(\\d+)\\s*weeks?',\n",
    "                r'Duration[:\\s]*([^\\n]+)'\n",
    "            ]\n",
    "            \n",
    "            for pattern in duration_patterns:\n",
    "                duration_match = re.search(pattern, container_text, re.IGNORECASE)\n",
    "                if duration_match:\n",
    "                    duration_raw = duration_match.group(0)\n",
    "                    break\n",
    "            \n",
    "            skills_raw = \"\"\n",
    "            skills_patterns = [\n",
    "                r'Skills?[:\\s]*([^\\n]+)',\n",
    "                r'Requirements?[:\\s]*([^\\n]+)',\n",
    "                r'Tools?[:\\s]*([^\\n]+)'\n",
    "            ]\n",
    "            \n",
    "            for pattern in skills_patterns:\n",
    "                skills_match = re.search(pattern, container_text, re.IGNORECASE)\n",
    "                if skills_match:\n",
    "                    skills_raw = skills_match.group(1).strip()\n",
    "                    break\n",
    "            \n",
    "            intern_type = \"Unpaid\" if stipend_numeric == 0 else \"Paid\"\n",
    "            if \"performance\" in stipend_raw.lower():\n",
    "                intern_type = \"Performance-based\"\n",
    "            \n",
    "            domain_category = self.categorize_domain(job_title, skills_raw)\n",
    "            \n",
    "            result = {\n",
    "                'platform': 'Internshala',\n",
    "                'company_name': company_name,\n",
    "                'job_title': job_title,\n",
    "                'domain_category': domain_category,\n",
    "                'skills_required': skills_raw,\n",
    "                'location': location,\n",
    "                'stipend': stipend_numeric,\n",
    "                'stipend_raw': stipend_raw,\n",
    "                'duration_months': self.parse_duration(duration_raw),\n",
    "                'duration_raw': duration_raw,\n",
    "                'posted_on': datetime.now().strftime('%Y-%m-%d'),\n",
    "                'intern_type': intern_type,\n",
    "                'location_type': self.categorize_location(location)\n",
    "            }\n",
    "            \n",
    "            if (company_name != \"Not specified\" or job_title != \"Not specified\") and len(container_text) > 50:\n",
    "                return result\n",
    "            else:\n",
    "                return None\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing internship container: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def parse_duration(self, duration_text):\n",
    "        \"\"\"Parse duration text to months\"\"\"\n",
    "        if not duration_text:\n",
    "            return 0\n",
    "        \n",
    "        months_match = re.search(r'(\\d+)\\s*month', duration_text.lower())\n",
    "        if months_match:\n",
    "            return int(months_match.group(1))\n",
    "        \n",
    "        weeks_match = re.search(r'(\\d+)\\s*week', duration_text.lower())\n",
    "        if weeks_match:\n",
    "            return round(int(weeks_match.group(1)) / 4.33, 1)  \n",
    "        \n",
    "        return 0\n",
    "    \n",
    "    def categorize_location(self, location):\n",
    "        \"\"\"Categorize location type\"\"\"\n",
    "        location_lower = location.lower()\n",
    "        if 'remote' in location_lower or 'work from home' in location_lower:\n",
    "            return 'Remote'\n",
    "        elif 'hybrid' in location_lower:\n",
    "            return 'Hybrid'\n",
    "        else:\n",
    "            return 'Onsite'\n",
    "    \n",
    "    def scrape_linkedin_internships(self, max_results=50):\n",
    "        \"\"\"Scrape LinkedIn internships (basic approach)\"\"\"\n",
    "        print(\"LinkedIn scraping requires more complex setup due to login requirements\")\n",
    "        print(\"For now, focusing on Internshala. You can extend this for LinkedIn later.\")\n",
    "        pass\n",
    "    \n",
    "    def clean_and_structure_data(self):\n",
    "        \"\"\"Clean and structure the scraped data\"\"\"\n",
    "        if not self.internships_data:\n",
    "            print(\"No data to clean\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        df = pd.DataFrame(self.internships_data)\n",
    "        \n",
    "        df['skills_required'] = df['skills_required'].apply(self.clean_skills)\n",
    "        \n",
    "        df = df.drop_duplicates(subset=['company_name', 'job_title', 'location'], keep='first')\n",
    "        \n",
    "        df['skills_count'] = df['skills_required'].apply(lambda x: len(x.split(',')) if x else 0)\n",
    "        df['has_stipend'] = df['stipend'] > 0\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def clean_skills(self, skills_text):\n",
    "        \"\"\"Clean and standardize skills text\"\"\"\n",
    "        if not skills_text or skills_text == \"Not specified\":\n",
    "            return \"\"\n",
    "        \n",
    "        skill_mappings = {\n",
    "            'photoshop': 'Adobe Photoshop',\n",
    "            'illustrator': 'Adobe Illustrator',\n",
    "            'ms office': 'Microsoft Office',\n",
    "            'excel': 'Microsoft Excel',\n",
    "            'powerpoint': 'Microsoft PowerPoint',\n",
    "            'word': 'Microsoft Word'\n",
    "        }\n",
    "        \n",
    "        skills = [skill.strip() for skill in skills_text.split(',')]\n",
    "        cleaned_skills = []\n",
    "        \n",
    "        for skill in skills:\n",
    "            skill_lower = skill.lower()\n",
    "            mapped_skill = skill_mappings.get(skill_lower, skill)\n",
    "            if len(mapped_skill) > 1:  \n",
    "                cleaned_skills.append(mapped_skill)\n",
    "        \n",
    "        return ', '.join(cleaned_skills[:5])  \n",
    "    \n",
    "    def save_data(self, filename='internship_data.csv'):\n",
    "        \"\"\"Save cleaned data to CSV\"\"\"\n",
    "        df = self.clean_and_structure_data()\n",
    "        if not df.empty:\n",
    "            df.to_csv(filename, index=False)\n",
    "            print(f\"Data saved to {filename}\")\n",
    "            print(f\"Total internships scraped: {len(df)}\")\n",
    "            return df\n",
    "        else:\n",
    "            print(\"No data to save\")\n",
    "            return pd.DataFrame()\n",
    "    \n",
    "    def get_summary_stats(self):\n",
    "        \"\"\"Get basic summary statistics\"\"\"\n",
    "        df = self.clean_and_structure_data()\n",
    "        if df.empty:\n",
    "            return \"No data available\"\n",
    "        \n",
    "        stats = {\n",
    "            'Total Internships': len(df),\n",
    "            'Paid Internships': len(df[df['has_stipend'] == True]),\n",
    "            'Unpaid Internships': len(df[df['has_stipend'] == False]),\n",
    "            'Average Stipend (Paid)': df[df['stipend'] > 0]['stipend'].mean(),\n",
    "            'Top Domains': df['domain_category'].value_counts().head(5).to_dict(),\n",
    "            'Location Types': df['location_type'].value_counts().to_dict()\n",
    "        }\n",
    "        \n",
    "        return stats\n",
    "\n",
    "def main():\n",
    "    scraper = InternshipScraper()\n",
    "    scraper.scrape_internshala(max_pages=5) \n",
    "    df = scraper.save_data('internship_market_data.csv')\n",
    "    stats = scraper.get_summary_stats()\n",
    "    print(\"\\n=== SCRAPING SUMMARY ===\")\n",
    "\n",
    "    for key, value in stats.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "    \n",
    "    return df\n",
    "if __name__ == \"__main__\":\n",
    "    df = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e20be07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import re\n",
    "from datetime import datetime\n",
    "import json\n",
    "from urllib.parse import urljoin, urlparse\n",
    "import random\n",
    "\n",
    "class InternshipScraper:\n",
    "    def __init__(self):\n",
    "        self.headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
    "            'Accept-Language': 'en-US,en;q=0.5',\n",
    "            'Accept-Encoding': 'gzip, deflate',\n",
    "            'Connection': 'keep-alive',\n",
    "            'Upgrade-Insecure-Requests': '1'\n",
    "        }\n",
    "        self.session = requests.Session()\n",
    "        self.session.headers.update(self.headers)\n",
    "        self.internships_data = []\n",
    "        \n",
    "    def extract_stipend_value(self, stipend_text):\n",
    "        \"\"\"Extract numeric stipend value from text\"\"\"\n",
    "        if not stipend_text or 'unpaid' in stipend_text.lower():\n",
    "            return 0\n",
    "        \n",
    "        numbers = re.findall(r'[\\d,]+', stipend_text.replace(',', ''))\n",
    "        if numbers:\n",
    "            nums = [int(n) for n in numbers]\n",
    "            return sum(nums) / len(nums)\n",
    "        return 0\n",
    "    \n",
    "    def categorize_domain(self, title, skills=\"\"):\n",
    "        \"\"\"Categorize internship into domain based on title and skills\"\"\"\n",
    "        title_lower = title.lower()\n",
    "        skills_lower = skills.lower() if skills else \"\"\n",
    "        combined = f\"{title_lower} {skills_lower}\"\n",
    "        \n",
    "        categories = {\n",
    "            'Tech': ['software', 'developer', 'programming', 'python', 'java', 'react', 'node', 'full stack', 'backend', 'frontend', 'web dev', 'app dev', 'data science', 'machine learning', 'ai', 'ml'],\n",
    "            'Design': ['ui', 'ux', 'graphic', 'design', 'figma', 'photoshop', 'illustrator', 'canva', 'creative'],\n",
    "            'Marketing': ['marketing', 'digital marketing', 'seo', 'social media', 'content', 'advertising', 'brand', 'campaign'],\n",
    "            'Sales': ['sales', 'business development', 'bd', 'lead generation', 'customer acquisition'],\n",
    "            'Finance': ['finance', 'accounting', 'financial', 'investment', 'banking', 'audit'],\n",
    "            'Operations': ['operations', 'logistics', 'supply chain', 'project management', 'admin'],\n",
    "            'HR': ['human resource', 'hr', 'recruitment', 'talent acquisition', 'people'],\n",
    "            'Content': ['content writing', 'copywriting', 'blog', 'writer', 'journalism', 'editor'],\n",
    "            'Research': ['research', 'analyst', 'market research', 'data analyst', 'business analyst']\n",
    "        }\n",
    "        \n",
    "        for category, keywords in categories.items():\n",
    "            if any(keyword in combined for keyword in keywords):\n",
    "                return category\n",
    "        \n",
    "        return 'Other'\n",
    "    \n",
    "    def extract_posted_company_website(self, container, internship_detail_url=None):\n",
    "        \"\"\"Extract the actual company website posted on Internshala\"\"\"\n",
    "        website_url = None\n",
    "        \n",
    "        website_selectors = [\n",
    "            'a[href*=\"company_website\"]',\n",
    "            'a[title*=\"website\"]',\n",
    "            'a[title*=\"Website\"]',\n",
    "            '.company-details a',\n",
    "            '.company-info a',\n",
    "            '.company-website',\n",
    "            '.website-link',\n",
    "            'a[href]:not([href*=\"internshala.com\"]):not([href*=\"javascript\"]):not([href*=\"mailto\"])'\n",
    "        ]\n",
    "        \n",
    "        for selector in website_selectors:\n",
    "            try:\n",
    "                links = container.select(selector)\n",
    "                for link in links:\n",
    "                    href = link.get('href', '')\n",
    "                    title = link.get('title', '').lower()\n",
    "                    text = link.get_text(strip=True).lower()\n",
    "                    \n",
    "                    if href and self.is_valid_company_website(href):\n",
    "                        if any(keyword in f\"{title} {text}\" for keyword in ['website', 'company', 'visit', 'www', 'site']):\n",
    "                            website_url = self.clean_website_url(href)\n",
    "                            break\n",
    "                        elif self.is_external_domain(href):\n",
    "                            website_url = self.clean_website_url(href)\n",
    "                            break\n",
    "                if website_url:\n",
    "                    break\n",
    "            except Exception as e:\n",
    "                continue\n",
    "        \n",
    "        if not website_url and internship_detail_url:\n",
    "            try:\n",
    "                website_url = self.extract_from_detail_page(internship_detail_url)\n",
    "            except Exception as e:\n",
    "                print(f\"Error extracting from detail page: {e}\")\n",
    "        \n",
    "        if not website_url:\n",
    "            website_url = self.extract_from_company_profile(container)\n",
    "        \n",
    "        return website_url\n",
    "    \n",
    "    def is_valid_company_website(self, url):\n",
    "        \"\"\"Check if URL looks like a valid company website\"\"\"\n",
    "        if not url:\n",
    "            return False\n",
    "        \n",
    "        if 'internshala.com' in url:\n",
    "            return False\n",
    "        \n",
    "        if url.startswith(('mailto:', 'tel:', 'javascript:')):\n",
    "            return False\n",
    "        \n",
    "        social_domains = ['facebook.com', 'twitter.com', 'linkedin.com', 'instagram.com', 'youtube.com']\n",
    "        if any(domain in url for domain in social_domains):\n",
    "            return False\n",
    "        \n",
    "        if any(pattern in url for pattern in ['http://', 'https://', 'www.', '.com', '.in', '.org', '.net']):\n",
    "            return True\n",
    "        \n",
    "        return False\n",
    "    \n",
    "    def is_external_domain(self, url):\n",
    "        \"\"\"Check if URL is an external domain (not Internshala)\"\"\"\n",
    "        try:\n",
    "            parsed = urlparse(url)\n",
    "            domain = parsed.netloc.lower()\n",
    "            return domain and 'internshala.com' not in domain\n",
    "        except:\n",
    "            return False\n",
    "    \n",
    "    def clean_website_url(self, url):\n",
    "        \"\"\"Clean and normalize website URL\"\"\"\n",
    "        if not url:\n",
    "            return None\n",
    "        \n",
    "        if not url.startswith(('http://', 'https://')):\n",
    "            if url.startswith('www.'):\n",
    "                url = 'https://' + url\n",
    "            elif '.' in url:\n",
    "                url = 'https://' + url\n",
    "        \n",
    "        try:\n",
    "            parsed = urlparse(url)\n",
    "            clean_url = f\"{parsed.scheme}://{parsed.netloc}{parsed.path}\"\n",
    "            return clean_url.rstrip('/')\n",
    "        except:\n",
    "            return url\n",
    "    \n",
    "    def extract_from_detail_page(self, detail_url):\n",
    "        \"\"\"Extract company website from internship detail page\"\"\"\n",
    "        try:\n",
    "            if not detail_url.startswith('http'):\n",
    "                    detail_url = 'https://internshala.com' + detail_url\n",
    "            \n",
    "            response = self.session.get(detail_url, timeout=10)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            detail_selectors = [\n",
    "                '.company-details a[href*=\"http\"]',\n",
    "                '.about-company a[href*=\"http\"]',\n",
    "                '.company-info a[href*=\"http\"]',\n",
    "                'a[title*=\"website\"]',\n",
    "                'a[title*=\"Website\"]',\n",
    "                '.company-website',\n",
    "                'a[href]:not([href*=\"internshala.com\"])'\n",
    "            ]\n",
    "            \n",
    "            for selector in detail_selectors:\n",
    "                try:\n",
    "                    links = soup.select(selector)\n",
    "                    for link in links:\n",
    "                        href = link.get('href', '')\n",
    "                        if self.is_valid_company_website(href):\n",
    "                            return self.clean_website_url(href)\n",
    "                except:\n",
    "                    continue\n",
    "            \n",
    "            return None\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching detail page {detail_url}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def extract_from_company_profile(self, container):\n",
    "        \"\"\"Extract website from company profile information\"\"\"\n",
    "        try:\n",
    "            profile_links = container.select('a[href*=\"company\"]')\n",
    "            \n",
    "            for link in profile_links:\n",
    "                href = link.get('href', '')\n",
    "                if 'company' in href and 'internshala.com' in href:\n",
    "                    try:\n",
    "                        company_url = urljoin('https://internshala.com', href)\n",
    "                        response = self.session.get(company_url, timeout=10)\n",
    "                        response.raise_for_status()\n",
    "                        \n",
    "                        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "                        \n",
    "                        website_selectors = [\n",
    "                            '.company-website a',\n",
    "                            '.website a',\n",
    "                            'a[title*=\"website\"]',\n",
    "                            'a[href*=\"http\"]:not([href*=\"internshala.com\"])'\n",
    "                        ]\n",
    "                        \n",
    "                        for selector in website_selectors:\n",
    "                            website_links = soup.select(selector)\n",
    "                            for w_link in website_links:\n",
    "                                w_href = w_link.get('href', '')\n",
    "                                if self.is_valid_company_website(w_href):\n",
    "                                    return self.clean_website_url(w_href)\n",
    "                    except:\n",
    "                        continue\n",
    "            \n",
    "            return None\n",
    "            \n",
    "        except Exception as e:\n",
    "            return None\n",
    "    \n",
    "    def get_internship_detail_url(self, container):\n",
    "        \"\"\"Extract the detail page URL for the internship\"\"\"\n",
    "        try:\n",
    "            detail_selectors = [\n",
    "                'a[href*=\"internship\"]',\n",
    "                '.view_detail_button a',\n",
    "                '.view-details a',\n",
    "                'a[title*=\"detail\"]',\n",
    "                'a[href*=\"/internship/\"]'\n",
    "            ]\n",
    "            \n",
    "            for selector in detail_selectors:\n",
    "                links = container.select(selector)\n",
    "                for link in links:\n",
    "                    href = link.get('href', '')\n",
    "                    if 'internship' in href and ('detail' in href or 'view' in href.lower()):\n",
    "                        return href\n",
    "            \n",
    "            return None\n",
    "            \n",
    "        except Exception as e:\n",
    "            return None\n",
    "    \n",
    "    def verify_website_exists(self, url):\n",
    "        \"\"\"Verify if a website URL exists and is accessible\"\"\"\n",
    "        if not url:\n",
    "            return False, \"No URL provided\"\n",
    "        \n",
    "        try:\n",
    "            response = requests.head(url, timeout=10, allow_redirects=True)\n",
    "            if response.status_code == 200:\n",
    "                return True, \"Valid\"\n",
    "            elif response.status_code == 404:\n",
    "                return False, \"Not Found\"\n",
    "            else:\n",
    "                return False, f\"HTTP {response.status_code}\"\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            return False, f\"Connection Error: {str(e)[:50]}\"\n",
    "    \n",
    "    def scrape_internshala(self, max_pages=5, verify_websites=False):\n",
    "        \"\"\"Scrape internships from Internshala\"\"\"\n",
    "        print(\"Starting Internshala scraping...\")\n",
    "        base_url = \"https://internshala.com/internships\"\n",
    "        \n",
    "        for page in range(1, max_pages + 1):\n",
    "            try:\n",
    "                print(f\"Scraping page {page}...\")\n",
    "                url = f\"{base_url}/page-{page}\" if page > 1 else base_url\n",
    "                \n",
    "                response = self.session.get(url, timeout=10)\n",
    "                response.raise_for_status()\n",
    "                \n",
    "                soup = BeautifulSoup(response.content, 'html.parser')\n",
    "                \n",
    "                internship_containers = []\n",
    "                \n",
    "                selectors_to_try = [\n",
    "                    'div.internship_meta',\n",
    "                    'div[class*=\"internship\"]',\n",
    "                    'div.individual_internship',\n",
    "                    'div.internship-item',\n",
    "                    'div[data-internship-id]',\n",
    "                    'div.container-fluid.individual_internship',\n",
    "                    'div.view_detail_button'\n",
    "                ]\n",
    "                \n",
    "                for selector in selectors_to_try:\n",
    "                    try:\n",
    "                        containers = soup.select(selector)\n",
    "                        if containers:\n",
    "                            internship_containers = containers\n",
    "                            print(f\"Found {len(containers)} internships using selector: {selector}\")\n",
    "                            break\n",
    "                    except:\n",
    "                        continue\n",
    "                \n",
    "                if not internship_containers:\n",
    "                    all_divs = soup.find_all('div')\n",
    "                    for div in all_divs:\n",
    "                        if div.get_text() and any(keyword in div.get_text().lower() for keyword in ['internship', 'stipend', 'apply by']):\n",
    "                            text = div.get_text().lower()\n",
    "                            if 'stipend' in text and ('month' in text or 'week' in text):\n",
    "                                internship_containers.append(div)\n",
    "                    \n",
    "                    internship_containers = [div for div in internship_containers if len(div.get_text()) > 100][:20]\n",
    "                    print(f\"Found {len(internship_containers)} internships using broad search\")\n",
    "                \n",
    "                if not internship_containers:\n",
    "                    print(f\"No internships found on page {page}\")\n",
    "                    with open(f'debug_page_{page}.html', 'w', encoding='utf-8') as f:\n",
    "                        f.write(soup.prettify())\n",
    "                    print(f\"Saved HTML to debug_page_{page}.html for inspection\")\n",
    "                    break\n",
    "                \n",
    "                for i, container in enumerate(internship_containers):\n",
    "                    try:\n",
    "                        print(f\"Processing internship {i+1}/{len(internship_containers)}\")\n",
    "                        internship_data = self.extract_internshala_data(container, verify_websites)\n",
    "                        if internship_data:\n",
    "                            self.internships_data.append(internship_data)\n",
    "                            print(f\"Successfully extracted: {internship_data['job_title']} at {internship_data['company_name']}\")\n",
    "                            if internship_data['company_website']:\n",
    "                                print(f\"  Website found: {internship_data['company_website']}\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error extracting internship data {i+1}: {e}\")\n",
    "                        continue\n",
    "                \n",
    "                print(f\"Page {page} complete. Total internships so far: {len(self.internships_data)}\")\n",
    "                \n",
    "                time.sleep(random.uniform(2, 4))\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error scraping page {page}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        print(f\"Scraped {len(self.internships_data)} internships from Internshala\")\n",
    "    \n",
    "    def extract_internshala_data(self, container, verify_websites=False):\n",
    "        \"\"\"Extract data from individual internship container\"\"\"\n",
    "        try:\n",
    "            container_text = container.get_text()\n",
    "            \n",
    "            company_name = \"Not specified\"\n",
    "            company_selectors = [\n",
    "                'a.link_display_like_text',\n",
    "                'a[href*=\"company\"]',\n",
    "                'h4 a',\n",
    "                'h5 a',\n",
    "                '.company-name'\n",
    "            ]\n",
    "            \n",
    "            for selector in company_selectors:\n",
    "                company_elem = container.select_one(selector)\n",
    "                if company_elem:\n",
    "                    company_name = company_elem.get_text(strip=True)\n",
    "                    break\n",
    "            \n",
    "            if company_name == \"Not specified\":\n",
    "                company_match = re.search(r'Company[:\\s]*([^\\n]+)', container_text, re.IGNORECASE)\n",
    "                if company_match:\n",
    "                    company_name = company_match.group(1).strip()\n",
    "            \n",
    "            detail_url = self.get_internship_detail_url(container)\n",
    "            \n",
    "            company_website = self.extract_posted_company_website(container, detail_url)\n",
    "            \n",
    "\n",
    "            website_status = \"Not Checked\"\n",
    "            if verify_websites and company_website:\n",
    "                is_valid, status = self.verify_website_exists(company_website)\n",
    "                website_status = status\n",
    "                print(f\"Website verification for {company_name}: {status}\")\n",
    "            elif company_website:\n",
    "                website_status = \"Found but Not Verified\"\n",
    "            else:\n",
    "                website_status = \"No Website Posted\"\n",
    "            \n",
    "            \n",
    "            job_title = \"Not specified\"\n",
    "            title_selectors = [\n",
    "                'h3.heading_4_5',\n",
    "                'h4.heading_4_5',\n",
    "                'h3',\n",
    "                'h4',\n",
    "                '.job-title',\n",
    "                '.internship-title'\n",
    "            ]\n",
    "            \n",
    "            for selector in title_selectors:\n",
    "                title_elem = container.select_one(selector)\n",
    "                if title_elem:\n",
    "                    job_title = title_elem.get_text(strip=True)\n",
    "                    break\n",
    "            \n",
    "            \n",
    "            if job_title == \"Not specified\":\n",
    "                lines = container_text.split('\\n')\n",
    "                for line in lines:\n",
    "                    if 'intern' in line.lower() and len(line.strip()) < 100:\n",
    "                        job_title = line.strip()\n",
    "                        break\n",
    "            \n",
    "           \n",
    "            location = \"Not specified\"\n",
    "            location_selectors = [\n",
    "                'a[href*=\"location\"]',\n",
    "                '.location',\n",
    "                '.internship-location'\n",
    "            ]\n",
    "            \n",
    "            for selector in location_selectors:\n",
    "                location_elem = container.select_one(selector)\n",
    "                if location_elem:\n",
    "                    location = location_elem.get_text(strip=True)\n",
    "                    break\n",
    "            \n",
    "           \n",
    "            if location == \"Not specified\":\n",
    "                location_patterns = [\n",
    "                    r'Location[:\\s]*([^\\n]+)',\n",
    "                    r'Work from home|Remote',\n",
    "                    r'([A-Z][a-z]+(?:\\s+[A-Z][a-z]+)*),?\\s*(?:India|IN)?'\n",
    "                ]\n",
    "                for pattern in location_patterns:\n",
    "                    location_match = re.search(pattern, container_text, re.IGNORECASE)\n",
    "                    if location_match:\n",
    "                        location = location_match.group(1).strip() if location_match.groups() else location_match.group(0)\n",
    "                        break\n",
    "            \n",
    "            \n",
    "            stipend_raw = \"Not specified\"\n",
    "            stipend_selectors = [\n",
    "                'span.stipend',\n",
    "                '.stipend',\n",
    "                '.salary',\n",
    "                '.compensation'\n",
    "            ]\n",
    "            \n",
    "            for selector in stipend_selectors:\n",
    "                stipend_elem = container.select_one(selector)\n",
    "                if stipend_elem:\n",
    "                    stipend_raw = stipend_elem.get_text(strip=True)\n",
    "                    break\n",
    "            \n",
    "            if stipend_raw == \"Not specified\":\n",
    "                stipend_patterns = [\n",
    "                    r'â‚¹\\s*[\\d,]+(?:\\s*-\\s*â‚¹?\\s*[\\d,]+)?',\n",
    "                    r'Stipend[:\\s]*([^\\n]+)',\n",
    "                    r'(\\d+(?:,\\d+)*)\\s*(?:per month|/month)',\n",
    "                    r'Unpaid',\n",
    "                    r'Performance based'\n",
    "                ]\n",
    "                for pattern in stipend_patterns:\n",
    "                    stipend_match = re.search(pattern, container_text, re.IGNORECASE)\n",
    "                    if stipend_match:\n",
    "                        stipend_raw = stipend_match.group(0)\n",
    "                        break\n",
    "            \n",
    "            stipend_numeric = self.extract_stipend_value(stipend_raw)\n",
    "            \n",
    "            duration_raw = \"Not specified\"\n",
    "            duration_patterns = [\n",
    "                r'(\\d+)\\s*months?',\n",
    "                r'(\\d+)\\s*weeks?',\n",
    "                r'Duration[:\\s]*([^\\n]+)'\n",
    "            ]\n",
    "            \n",
    "            for pattern in duration_patterns:\n",
    "                duration_match = re.search(pattern, container_text, re.IGNORECASE)\n",
    "                if duration_match:\n",
    "                    duration_raw = duration_match.group(0)\n",
    "                    break\n",
    "            \n",
    "            skills_raw = \"\"\n",
    "            \n",
    "            skills_container = container.select_one('.round_tabs_container')\n",
    "            if skills_container:\n",
    "                skills_raw = skills_container.get_text(' ', strip=True)\n",
    "            else:\n",
    "                \n",
    "                skill_elements = container.select('.round_tabs')\n",
    "                if skill_elements:\n",
    "                    skills_raw = ', '.join([skill.get_text(strip=True) for skill in skill_elements])\n",
    "                else:\n",
    "                    \n",
    "                    skills_patterns = [\n",
    "                        r'Skills?[:\\s]*([^\\n]+)',\n",
    "                        r'Requirements?[:\\s]*([^\\n]+)',\n",
    "                        r'Tools?[:\\s]*([^\\n]+)',\n",
    "                        r'Qualifications?[:\\s]*([^\\n]+)'\n",
    "                    ]\n",
    "                    \n",
    "                    for pattern in skills_patterns:\n",
    "                        skills_match = re.search(pattern, container_text, re.IGNORECASE)\n",
    "                        if skills_match:\n",
    "                            skills_raw = skills_match.group(1).strip()\n",
    "                            break\n",
    "            \n",
    "            \n",
    "            if skills_raw:\n",
    "                skills_raw = self.clean_skills(skills_raw)\n",
    "            \n",
    "           \n",
    "            intern_type = \"Unpaid\" if stipend_numeric == 0 else \"Paid\"\n",
    "            if \"performance\" in stipend_raw.lower():\n",
    "                intern_type = \"Performance-based\"\n",
    "            \n",
    "            domain_category = self.categorize_domain(job_title, skills_raw)\n",
    "            \n",
    "            result = {\n",
    "                'platform': 'Internshala',\n",
    "                'company_name': company_name,\n",
    "                'job_title': job_title,\n",
    "                'domain_category': domain_category,\n",
    "                'skills_required': skills_raw,\n",
    "                'location': location,\n",
    "                'stipend': stipend_numeric,\n",
    "                'stipend_raw': stipend_raw,\n",
    "                'duration_months': self.parse_duration(duration_raw),\n",
    "                'duration_raw': duration_raw,\n",
    "                'posted_on': datetime.now().strftime('%Y-%m-%d'),\n",
    "                'intern_type': intern_type,\n",
    "                'location_type': self.categorize_location(location),\n",
    "                'company_website': company_website,\n",
    "                'website_status': website_status,\n",
    "                'detail_url': detail_url\n",
    "            }\n",
    "            \n",
    "           \n",
    "            if (company_name != \"Not specified\" or job_title != \"Not specified\") and len(container_text) > 50:\n",
    "                return result\n",
    "            else:\n",
    "                return None\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing internship container: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def parse_duration(self, duration_text):\n",
    "        \"\"\"Parse duration text to months\"\"\"\n",
    "        if not duration_text:\n",
    "            return 0\n",
    "        \n",
    "     \n",
    "        months_match = re.search(r'(\\d+)\\s*month', duration_text.lower())\n",
    "        if months_match:\n",
    "            return int(months_match.group(1))\n",
    "        \n",
    "      \n",
    "        weeks_match = re.search(r'(\\d+)\\s*week', duration_text.lower())\n",
    "        if weeks_match:\n",
    "            return round(int(weeks_match.group(1)) / 4.33, 1)  \n",
    "        \n",
    "        return 0\n",
    "    \n",
    "    def categorize_location(self, location):\n",
    "        \"\"\"Categorize location type\"\"\"\n",
    "        location_lower = location.lower()\n",
    "        if 'remote' in location_lower or 'work from home' in location_lower:\n",
    "            return 'Remote'\n",
    "        elif 'hybrid' in location_lower:\n",
    "            return 'Hybrid'\n",
    "        else:\n",
    "            return 'Onsite'\n",
    "    \n",
    "    def update_existing_data_with_posted_websites(self, csv_file_path, verify_websites=True):\n",
    "        \"\"\"Update existing data with posted company websites from Internshala (not guessed ones)\"\"\"\n",
    "        try:\n",
    "            df = pd.read_csv(csv_file_path)\n",
    "            print(f\"Loaded {len(df)} existing records\")\n",
    "            \n",
    "            df['company_website'] = \"Re-scrape needed for posted websites\"\n",
    "            df['website_status'] = \"Need to re-scrape\"\n",
    "            \n",
    "            \n",
    "            output_file = csv_file_path.replace('.csv', '_with_posted_websites.csv')\n",
    "            df.to_csv(output_file, index=False)\n",
    "            print(f\"Updated data saved to {output_file}\")\n",
    "            print(\"Note: To get actual posted websites, please run a fresh scrape with the updated scraper\")\n",
    "            \n",
    "            return df\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing existing data: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def scrape_linkedin_internships(self, max_results=50):\n",
    "        \"\"\"Scrape LinkedIn internships (basic approach)\"\"\"\n",
    "        print(\"LinkedIn scraping requires more complex setup due to login requirements\")\n",
    "        print(\"For now, focusing on Internshala. You can extend this for LinkedIn later.\")\n",
    "        pass\n",
    "    \n",
    "    def clean_and_structure_data(self):\n",
    "        \"\"\"Clean and structure the scraped data\"\"\"\n",
    "        if not self.internships_data:\n",
    "            print(\"No data to clean\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        df = pd.DataFrame(self.internships_data)\n",
    "        \n",
    "        \n",
    "        df['skills_required'] = df['skills_required'].apply(self.clean_skills)\n",
    "        \n",
    "        df = df.drop_duplicates(subset=['company_name', 'job_title', 'location'], keep='first')\n",
    "        \n",
    "        df['skills_count'] = df['skills_required'].apply(lambda x: len(x.split(',')) if x else 0)\n",
    "        df['has_stipend'] = df['stipend'] > 0\n",
    "        df['has_posted_website'] = df['company_website'].notna() & (df['company_website'] != \"\")\n",
    "        df['website_valid'] = df['website_status'] == 'Valid'\n",
    "        df['website_found_but_not_verified'] = df['website_status'] == 'Found but Not Verified'\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def clean_skills(self, skills_text):\n",
    "        \"\"\"Clean and standardize skills text\"\"\"\n",
    "        if not skills_text or skills_text == \"Not specified\":\n",
    "            return \"\"\n",
    "        \n",
    "        skills_text = re.sub(r'[^\\w\\s,.-]', '', skills_text)\n",
    "        \n",
    "        skill_mappings = {\n",
    "            'photoshop': 'Adobe Photoshop',\n",
    "            'illustrator': 'Adobe Illustrator',\n",
    "            'ms office': 'Microsoft Office',\n",
    "            'excel': 'Microsoft Excel',\n",
    "            'powerpoint': 'Microsoft PowerPoint',\n",
    "            'word': 'Microsoft Word',\n",
    "            'js': 'JavaScript',\n",
    "            'html5': 'HTML',\n",
    "            'css3': 'CSS'\n",
    "        }\n",
    "        \n",
    "        skills = []\n",
    "        for part in skills_text.split(','):\n",
    "            part = part.strip()\n",
    "            if part:\n",
    "                for subpart in re.split(r'\\band\\b', part, flags=re.IGNORECASE):\n",
    "                    subpart = subpart.strip()\n",
    "                    if subpart:\n",
    "                        subpart_lower = subpart.lower()\n",
    "                        mapped_skill = skill_mappings.get(subpart_lower, subpart)\n",
    "                        if len(mapped_skill) > 1:  \n",
    "                            skills.append(mapped_skill)\n",
    "        \n",
    "        return ', '.join(sorted(set(skills[:10])))  \n",
    "    \n",
    "    def save_data(self, filename='internship_data_with_posted_websites.csv'):\n",
    "        \"\"\"Save cleaned data to CSV\"\"\"\n",
    "        df = self.clean_and_structure_data()\n",
    "        if not df.empty:\n",
    "            df.to_csv(filename, index=False)\n",
    "            print(f\"Data saved to {filename}\")\n",
    "            print(f\"Total internships scraped: {len(df)}\")\n",
    "            return df\n",
    "        else:\n",
    "            print(\"No data to save\")\n",
    "            return pd.DataFrame()\n",
    "    \n",
    "    def get_summary_stats(self):\n",
    "        \"\"\"Get basic summary statistics\"\"\"\n",
    "        df = self.clean_and_structure_data()\n",
    "        if df.empty:\n",
    "            return \"No data available\"\n",
    "        \n",
    "        stats = {\n",
    "            'Total Internships': len(df),\n",
    "            'Paid Internships': len(df[df['has_stipend'] == True]),\n",
    "            'Unpaid Internships': len(df[df['has_stipend'] == False]),\n",
    "            'Average Stipend (Paid)': df[df['stipend'] > 0]['stipend'].mean(),\n",
    "            'Top Domains': df['domain_category'].value_counts().head(5).to_dict(),\n",
    "            'Location Types': df['location_type'].value_counts().to_dict(),\n",
    "            'Companies with Posted Websites': len(df[df['has_posted_website'] == True]),\n",
    "            'Verified Valid Websites': len(df[df['website_valid'] == True]),\n",
    "            'Websites Found but Not Verified': len(df[df['website_found_but_not_verified'] == True]),\n",
    "            'No Website Posted': len(df[df['website_status'] == 'No Website Posted'])\n",
    "        }\n",
    "        \n",
    "        return stats\n",
    "\n",
    "def main():\n",
    "    scraper = InternshipScraper()\n",
    "    \n",
    "    \n",
    "    print(\"Scraping internships and extracting POSTED company websites...\")\n",
    "    scraper.scrape_internshala(max_pages=3, verify_websites=True) \n",
    "    df = scraper.save_data('internship_market_data_with_posted_websites.csv')\n",
    "    \n",
    "    stats = scraper.get_summary_stats()\n",
    "    print(\"\\n=== SCRAPING SUMMARY ===\")\n",
    "    for key, value in stats.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "    \n",
    "\n",
    "    if not df.empty:\n",
    "        websites_found = df[df['has_posted_website'] == True]\n",
    "        if not websites_found.empty:\n",
    "            print(f\"\\n=== EXAMPLE POSTED WEBSITES FOUND ===\")\n",
    "            for idx, row in websites_found.head(5).iterrows():\n",
    "                print(f\"{row['company_name']}: {row['company_website']} ({row['website_status']})\")\n",
    "        else:\n",
    "            print(\"\\nNo posted websites found in this scrape. This could mean:\")\n",
    "            print(\"1. Companies haven't posted their websites on their Internshala profiles\")\n",
    "            print(\"2. The website extraction selectors need adjustment for current Internshala structure\")\n",
    "            print(\"3. Websites might be in a different section that requires deeper page navigation\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    df = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27067070",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting job scraping with skills extraction...\n",
      "\n",
      "Scraping page 1...\n",
      "Found 50 jobs using selector: div.internship_meta\n",
      "Processing job 1/50\n",
      "Fetching skills from: https://internshala.com/internship/detail/part-time-public-speaking-theatre-coach-internship-in-multiple-locations-at-utsaah-learning-private-limited1750914763\n",
      "âœ“ Extracted: Public Speaking & Theatre Coach\n",
      "  Skills: Public Speaking...\n",
      "Processing job 2/50\n",
      "Fetching skills from: https://internshala.com/internship/detail/ca-articleship-internship-in-delhi-at-vimal-tandon-co1749710338\n",
      "âœ“ Extracted: CA Articleship\n",
      "  No skills found\n",
      "Processing job 3/50\n",
      "Fetching skills from: https://internshala.com/internship/detail/campus-ambassador-programme-at-billion-hearts1750314817\n",
      "âœ“ Extracted: Campus Ambassador\n",
      "  No skills found\n",
      "Processing job 4/50\n",
      "Fetching skills from: https://internshala.com/internship/detail/digital-marketing-internship-in-navi-mumbai-at-yours-faithfully-advisors-llp1750842293\n",
      "âœ“ Extracted: Digital Marketing\n",
      "  No skills found\n",
      "Processing job 5/50\n",
      "Fetching skills from: https://internshala.com/internship/detail/part-time-video-editor-photographer-female-internship-in-multiple-locations-at-astha-sharma1748331987\n",
      "âœ“ Extracted: Video Editor & Photographer (Female)\n",
      "  No skills found\n",
      "Processing job 6/50\n",
      "Fetching skills from: https://internshala.com/internship/detail/business-development-sales-male-internship-in-multiple-locations-at-tyresnmorecom1750846372\n",
      "âœ“ Extracted: Business Development (Sales) (Male)\n",
      "  Skills: MS-Excel Negotiation Problem Solving...\n",
      "Processing job 7/50\n",
      "Fetching skills from: https://internshala.com/internship/detail/inside-sales-internship-in-hyderabad-at-fosterate1749725152\n",
      "âœ“ Extracted: Inside Sales\n",
      "  Skills: Effective Communication...\n",
      "Processing job 8/50\n",
      "Fetching skills from: https://internshala.com/internship/detail/graphic-design-internship-in-lucknow-at-zframes-media1749040162\n",
      "âœ“ Extracted: Graphic Design\n",
      "  No skills found\n",
      "Processing job 9/50\n",
      "Fetching skills from: https://internshala.com/internship/detail/inside-sales-internship-in-pune-at-lime-learn-eduserv-private-limited1750756729\n",
      "âœ“ Extracted: Inside Sales\n",
      "  No skills found\n",
      "Processing job 10/50\n",
      "Fetching skills from: https://internshala.com/internship/detail/digital-marketing-internship-in-rajkot-at-72cubes1750652589\n",
      "âœ“ Extracted: Digital Marketing\n",
      "  No skills found\n",
      "Processing job 11/50\n",
      "Fetching skills from: https://internshala.com/internship/detail/hrbp-internship-in-jaipur-at-freecharge-payments-technology-private-limited1751026784\n",
      "âœ“ Extracted: HRBP\n",
      "  Skills: Effective Communication...\n",
      "Processing job 12/50\n",
      "Fetching skills from: https://internshala.com/internship/detail/video-editing-making-internship-in-multiple-locations-at-603-the-coworking-space-india1750880937\n",
      "âœ“ Extracted: Video Editing/Making\n",
      "  No skills found\n",
      "Processing job 13/50\n",
      "Fetching skills from: https://internshala.com/internship/detail/recruitment-internship-in-multiple-locations-at-bsa-corporation-ltd1751027296\n",
      "âœ“ Extracted: Recruitment\n",
      "  No skills found\n",
      "Processing job 14/50\n",
      "Fetching skills from: https://internshala.com/internship/detail/environmental-sciences-internship-in-mumbai-at-wwf-india1751001410\n",
      "âœ“ Extracted: Environmental Sciences\n",
      "  No skills found\n",
      "Processing job 15/50\n",
      "Fetching skills from: https://internshala.com/internship/detail/sales-and-marketing-internship-in-lucknow-at-shriram-life-insurance-company-limited1751008344\n",
      "âœ“ Extracted: Sales and Marketing\n",
      "  Skills: Financial planning...\n",
      "Processing job 16/50\n",
      "Fetching skills from: https://internshala.com/internship/detail/business-development-sales-internship-in-gurgaon-at-byld-group1750999855\n",
      "âœ“ Extracted: Business Development (Sales)\n",
      "  No skills found\n",
      "Processing job 17/50\n",
      "Fetching skills from: https://internshala.com/internship/detail/marketing-internship-in-mumbai-at-times-internet1750941954\n",
      "âœ“ Extracted: Marketing\n",
      "  Skills: English Proficiency (Spoken)...\n",
      "Processing job 18/50\n",
      "Fetching skills from: https://internshala.com/internship/detail/content-and-social-media-marketing-internship-in-mumbai-at-lakme-lever-private-limited1750940283\n",
      "âœ“ Extracted: Content and Social Media Marketing\n",
      "  No skills found\n",
      "Processing job 19/50\n",
      "Fetching skills from: https://internshala.com/internship/detail/business-research-internship-in-noida-at-brandwagon-the-financial-express1750934825\n",
      "âœ“ Extracted: Business Research\n",
      "  Skills: Business Research Data Analysis Lead Generation...\n",
      "Processing job 20/50\n",
      "Fetching skills from: https://internshala.com/internship/detail/human-resources-hr-internship-in-mumbai-at-godrej-infotech1750934693\n",
      "âœ“ Extracted: Human Resources (HR)\n",
      "  Skills: Effective Communication MS-Office...\n",
      "Processing job 21/50\n",
      "Fetching skills from: https://internshala.com/internship/detail/video-editing-making-internship-in-chennai-at-amnet-systems-private-limited1750934097\n",
      "âœ“ Extracted: Video Editing/Making\n",
      "  Skills: Video Editing Video Making...\n",
      "Processing job 22/50\n",
      "Fetching skills from: https://internshala.com/internship/detail/business-research-internship-in-noida-at-financial-express-indian-express-group1750923966\n",
      "âœ“ Extracted: Business Research\n",
      "  No skills found\n",
      "Processing job 23/50\n",
      "Fetching skills from: https://internshala.com/internship/detail/business-development-sales-internship-in-multiple-locations-at-urban-company1750918606\n",
      "âœ“ Extracted: Business Development (Sales)\n",
      "  Skills: MS-Excel...\n",
      "Processing job 24/50\n",
      "Fetching skills from: https://internshala.com/internship/detail/work-from-home-financial-stock-market-research-internship-at-focus-wise-solution1750922069\n",
      "âœ“ Extracted: Financial Stock Market Research\n",
      "  No skills found\n",
      "Processing job 25/50\n",
      "Fetching skills from: https://internshala.com/internship/detail/video-production-internship-in-bangalore-at-nobroker1750920136\n",
      "âœ“ Extracted: Video Production\n",
      "  No skills found\n",
      "Processing job 26/50\n",
      "Fetching skills from: https://internshala.com/internship/detail/search-engine-optimization-seo-internship-in-bangalore-at-nobroker1750915586\n",
      "âœ“ Extracted: Search Engine Optimization (SEO)\n",
      "  No skills found\n",
      "Processing job 27/50\n",
      "Fetching skills from: https://internshala.com/internship/detail/sales-and-marketing-internship-in-multiple-locations-at-wheebox1750834664\n",
      "âœ“ Extracted: Sales and Marketing\n",
      "  Skills: B2B Sales...\n",
      "Processing job 28/50\n",
      "Fetching skills from: https://internshala.com/internship/detail/graphic-design-internship-in-ahmedabad-at-petpooja1750841681\n",
      "âœ“ Extracted: Graphic Design\n",
      "  Skills: Adobe Illustrator Adobe Photoshop...\n",
      "Processing job 29/50\n",
      "Fetching skills from: https://internshala.com/internship/detail/part-time-business-development-sales-internship-in-lucknow-at-shriram-life-insurance-company-limited1750839387\n",
      "âœ“ Extracted: Business Development (Sales)\n",
      "  Skills: English Proficiency (Written) Finance Marketing...\n",
      "Processing job 30/50\n",
      "Fetching skills from: https://internshala.com/internship/detail/sales-consultant-internship-in-multiple-locations-at-aditya-birla-fashion-retail-limited-abfrl1750839125\n",
      "âœ“ Extracted: Sales Consultant\n",
      "  Skills: Sales...\n",
      "Processing job 31/50\n",
      "Fetching skills from: https://internshala.com/internship/detail/work-from-home-stock-market-learning-outreach-internship-at-focus-wise-solution1750829610\n",
      "âœ“ Extracted: Stock Market Learning & Outreach\n",
      "  No skills found\n",
      "Processing job 32/50\n",
      "Fetching skills from: https://internshala.com/internship/detail/recruitment-internship-in-mumbai-at-kotak-mahindra-bank-limited1750829228\n",
      "âœ“ Extracted: Recruitment\n",
      "  Skills: MS-Excel MS-Office Recruitment...\n",
      "Processing job 33/50\n",
      "Fetching skills from: https://internshala.com/internship/detail/visual-design-internship-in-bangalore-at-more-retail-private-limited1750771370\n",
      "âœ“ Extracted: Visual Design\n",
      "  Skills: Adobe Illustrator Adobe Photoshop Figma...\n",
      "Processing job 34/50\n",
      "Fetching skills from: https://internshala.com/internship/detail/work-from-home-human-resources-hr-internship-at-focus-wise-solution1750767978\n",
      "âœ“ Extracted: Human Resources (HR)\n",
      "  No skills found\n",
      "Processing job 35/50\n",
      "Fetching skills from: https://internshala.com/internship/detail/mobile-app-development-internship-in-mumbai-at-thomas-cook1750767748\n",
      "âœ“ Extracted: Mobile App Development\n",
      "  Skills: Dart Figma Flutter JSON REST API...\n",
      "Processing job 36/50\n",
      "Fetching skills from: https://internshala.com/internship/detail/finance-accounting-internship-in-multiple-locations-at-thomas-cook1750759907\n",
      "âœ“ Extracted: Finance & Accounting\n",
      "  Skills: Accounting Financial Reporting...\n",
      "Processing job 37/50\n",
      "Fetching skills from: https://internshala.com/internship/detail/sampling-internship-in-mumbai-at-navneet-education-limited1750752010\n",
      "âœ“ Extracted: Sampling\n",
      "  No skills found\n",
      "Processing job 38/50\n",
      "Fetching skills from: https://internshala.com/internship/detail/business-development-sales-internship-in-mumbai-at-urban-company1750751324\n",
      "âœ“ Extracted: Business Development (Sales)\n",
      "  No skills found\n",
      "Processing job 39/50\n",
      "Fetching skills from: https://internshala.com/internship/detail/graphics-designer-internship-in-bangalore-at-sri-sri-tattva1750750226\n",
      "âœ“ Extracted: Graphics Designer\n",
      "  No skills found\n",
      "Processing job 40/50\n",
      "Fetching skills from: https://internshala.com/internship/detail/content-and-social-media-marketing-internship-in-mumbai-at-manasi-shinde1750747530\n",
      "âœ“ Extracted: Content and Social Media Marketing\n",
      "  Skills: Social Media Marketing...\n",
      "Processing job 41/50\n",
      "Fetching skills from: https://internshala.com/internship/detail/graphic-design-internship-in-mumbai-at-shemaroo-entertainment-limited1750744818\n",
      "âœ“ Extracted: Graphic Design\n",
      "  No skills found\n",
      "Processing job 42/50\n",
      "Fetching skills from: https://internshala.com/internship/detail/google-my-business-gmb-internship-in-multiple-locations-at-abhinav-tirkey1750741649\n",
      "âœ“ Extracted: Google My Business (GMB)\n",
      "  No skills found\n",
      "Processing job 43/50\n",
      "Fetching skills from: https://internshala.com/internship/detail/digital-marketing-internship-in-gurgaon-at-gr-brand1750680145\n",
      "âœ“ Extracted: Digital Marketing\n",
      "  No skills found\n",
      "Processing job 44/50\n",
      "Fetching skills from: https://internshala.com/internship/detail/human-resources-hr-manager-internship-in-chennai-at-protium1750683541\n",
      "âœ“ Extracted: Human Resources (HR) Manager\n",
      "  Skills: MS-Office...\n",
      "Processing job 45/50\n",
      "Fetching skills from: https://internshala.com/internship/detail/marketing-and-sales-internship-in-multiple-locations-at-reliance-nippon-life-insurance1750677677\n",
      "âœ“ Extracted: Marketing and Sales\n",
      "  No skills found\n",
      "Processing job 46/50\n",
      "Fetching skills from: https://internshala.com/internship/detail/human-resources-hr-internship-in-gurgaon-at-collegedekhocom1750667103\n",
      "âœ“ Extracted: Human Resources (HR)\n",
      "  No skills found\n",
      "Processing job 47/50\n",
      "Fetching skills from: https://internshala.com/internship/detail/logistics-and-operations-internship-in-mumbai-at-indian-gifts-portal1750672513\n",
      "âœ“ Extracted: Logistics And Operations\n",
      "  Skills: Logistics Management MS-Excel Operations...\n",
      "Processing job 48/50\n",
      "Fetching skills from: https://internshala.com/internship/detail/sales-and-marketing-manager-internship-in-multiple-locations-at-imarticus-learning1750662446\n",
      "âœ“ Extracted: Sales and Marketing Manager\n",
      "  No skills found\n",
      "Processing job 49/50\n",
      "Fetching skills from: https://internshala.com/internship/detail/work-from-home-content-writing-internship-at-marpu-foundation1750573008\n",
      "âœ“ Extracted: Content Writing\n",
      "  No skills found\n",
      "Processing job 50/50\n",
      "Fetching skills from: https://internshala.com/internship/detail/business-development-sales-internship-in-mumbai-at-fern-hotels-resorts1750417395\n",
      "âœ“ Extracted: Business Development (Sales)\n",
      "  No skills found\n",
      "Page 1 complete. Total jobs: 50\n",
      "\n",
      "Scraping page 2...\n",
      "Found 40 jobs using selector: div.internship_meta\n",
      "Processing job 1/40\n",
      "Fetching skills from: https://internshala.com/internship/detail/ui-ux-design-internship-in-multiple-locations-at-thomas-cook1750406572\n",
      "âœ“ Extracted: UI/UX Design\n",
      "  Skills: UI  UX Design Wireframing...\n",
      "Processing job 2/40\n",
      "Fetching skills from: https://internshala.com/internship/detail/sales-and-marketing-internship-in-multiple-locations-at-reliance-nippon-life-insurance1750395650\n",
      "âœ“ Extracted: Sales and Marketing\n",
      "  Skills: Effective Communication Lead Generation...\n",
      "Processing job 3/40\n",
      "Fetching skills from: https://internshala.com/internship/detail/video-editing-making-internship-in-bangalore-at-letstransport1750337901\n",
      "âœ“ Extracted: Video Editing/Making\n",
      "  Skills: Animation Canva Video Editing Video Making...\n",
      "Processing job 4/40\n",
      "Fetching skills from: https://internshala.com/internship/detail/chartered-accountancy-ca-internship-in-multiple-locations-at-jm-financial1750334417\n",
      "âœ“ Extracted: Chartered Accountancy (CA)\n",
      "  No skills found\n",
      "Processing job 5/40\n",
      "Fetching skills from: https://internshala.com/internship/detail/operations-internship-in-noida-at-decathlon-sport-india-private-limited1750318343\n",
      "âœ“ Extracted: Operations\n",
      "  Skills: MS-Excel MS-Office MS-Word...\n",
      "Processing job 6/40\n",
      "Fetching skills from: https://internshala.com/internship/detail/motion-graphics-internship-in-bangalore-at-more-retail-private-limited1750245760\n",
      "âœ“ Extracted: Motion Graphics\n",
      "  Skills: Motion Graphics...\n",
      "Processing job 7/40\n",
      "Fetching skills from: https://internshala.com/internship/detail/content-writing-internship-in-bangalore-at-nobroker1750244720\n",
      "âœ“ Extracted: Content Writing\n",
      "  No skills found\n",
      "Processing job 8/40\n",
      "Fetching skills from: https://internshala.com/internship/detail/marketing-automation-executive-internship-in-multiple-locations-at-thomas-cook1750235498\n",
      "âœ“ Extracted: Marketing Automation Executive\n",
      "  No skills found\n",
      "Processing job 9/40\n",
      "Fetching skills from: https://internshala.com/internship/detail/part-time-content-and-social-media-marketing-internship-in-kanpur-at-puja-gupta1750233800\n",
      "âœ“ Extracted: Content and Social Media Marketing\n",
      "  Skills: Social Media Marketing...\n",
      "Processing job 10/40\n",
      "Fetching skills from: https://internshala.com/internship/detail/part-time-computational-physics-chemistry-materials-science-internship-in-mumbai-at-bhabha-atomic-research-centre-barc1750227368\n",
      "âœ“ Extracted: Computational Physics/Chemistry/Materials Science\n",
      "  Skills: Chemistry Physics...\n",
      "Processing job 11/40\n",
      "Fetching skills from: https://internshala.com/internship/detail/law-legal-internship-in-bangalore-at-moveinsync-technology-solutions-private-limited1750164934\n",
      "âœ“ Extracted: Law/Legal\n",
      "  No skills found\n",
      "Processing job 12/40\n",
      "Fetching skills from: https://internshala.com/internship/detail/law-legal-internship-in-mumbai-at-europ-assistance1750055659\n",
      "âœ“ Extracted: Law/Legal\n",
      "  No skills found\n",
      "Processing job 13/40\n",
      "Fetching skills from: https://internshala.com/internship/detail/front-end-development-internship-in-delhi-at-ghar-pe-shiksha1750161842\n",
      "âœ“ Extracted: Front End Development\n",
      "  No skills found\n",
      "Processing job 14/40\n",
      "Fetching skills from: https://internshala.com/internship/detail/digital-media-intern-for-video-production-internship-in-bangalore-at-aster-dm-healthcare1750159805\n",
      "âœ“ Extracted: Digital Media Intern For Video Production\n",
      "  No skills found\n",
      "Processing job 15/40\n",
      "Fetching skills from: https://internshala.com/internship/detail/business-development-sales-internship-in-multiple-locations-at-confederation-of-indian-industry-cii1750140325\n",
      "âœ“ Extracted: Business Development (Sales)\n",
      "  No skills found\n",
      "Processing job 16/40\n",
      "Fetching skills from: https://internshala.com/internship/detail/sales-apprentice-internship-in-mumbai-at-halma1750150377\n",
      "âœ“ Extracted: Sales Apprentice\n",
      "  Skills: Effective Communication...\n",
      "Processing job 17/40\n",
      "Fetching skills from: https://internshala.com/internship/detail/work-from-home-part-time-law-legal-internship-at-s-singh1750144333\n",
      "âœ“ Extracted: Law/Legal\n",
      "  No skills found\n",
      "Processing job 18/40\n",
      "Fetching skills from: https://internshala.com/internship/detail/product-design-internship-in-bangalore-at-scaler1750143353\n",
      "âœ“ Extracted: Product Design\n",
      "  Skills: Figma UI  UX Design Visual Design...\n",
      "Processing job 19/40\n",
      "Fetching skills from: https://internshala.com/internship/detail/b2b-business-development-internship-in-bangalore-at-scaler1750142653\n",
      "âœ“ Extracted: B2B Business Development\n",
      "  Skills: B2B Sales Effective Communication Sales...\n",
      "Processing job 20/40\n",
      "Fetching skills from: https://internshala.com/internship/detail/creative-design-internship-in-mumbai-at-waaree-energies-limited1749811663\n",
      "âœ“ Extracted: Creative Design\n",
      "  No skills found\n",
      "Processing job 21/40\n",
      "Fetching skills from: https://internshala.com/internship/detail/digital-ad-sales-internship-in-noida-at-times-internet1749814936\n",
      "âœ“ Extracted: Digital Ad Sales\n",
      "  Skills: Effective Communication MS-Excel...\n",
      "Processing job 22/40\n",
      "Fetching skills from: https://internshala.com/internship/detail/hotel-managementchef-internship-in-mumbai-at-theobroma-foods-private-limited1749735667\n",
      "âœ“ Extracted: Hotel Management(Chef)\n",
      "  Skills: Culinary Arts...\n",
      "Processing job 23/40\n",
      "Fetching skills from: https://internshala.com/internship/detail/part-time-company-secretary-cs-internship-in-bangalore-at-motherhood-hospital1749643827\n",
      "âœ“ Extracted: Company Secretary (CS)\n",
      "  Skills: Financial Reporting GST...\n",
      "Processing job 24/40\n",
      "Fetching skills from: https://internshala.com/internship/detail/business-development-sales-internship-in-thane-at-ilux-electricals1749643234\n",
      "âœ“ Extracted: Business Development (Sales)\n",
      "  No skills found\n",
      "Processing job 25/40\n",
      "Fetching skills from: https://internshala.com/internship/detail/legal-research-internship-in-gurgaon-at-collegedekhocom1749637987\n",
      "âœ“ Extracted: Legal Research\n",
      "  No skills found\n",
      "Processing job 26/40\n",
      "Fetching skills from: https://internshala.com/internship/detail/inside-sales-internship-in-bangalore-at-unacademy-sorting-hat-technologies-private-limited1749628887\n",
      "âœ“ Extracted: Inside Sales\n",
      "  Skills: Private Limited)...\n",
      "Processing job 27/40\n",
      "Fetching skills from: https://internshala.com/internship/detail/sales-internship-in-mumbai-at-halma1749624507\n",
      "âœ“ Extracted: Sales\n",
      "  No skills found\n",
      "Processing job 28/40\n",
      "Fetching skills from: https://internshala.com/internship/detail/human-resources-hr-internship-in-mumbai-at-audi-mumbai-west1749557581\n",
      "âœ“ Extracted: Human Resources (HR)\n",
      "  No skills found\n",
      "Processing job 29/40\n",
      "Fetching skills from: https://internshala.com/internship/detail/video-editing-making-internship-in-bangalore-at-zeta-india1749541693\n",
      "âœ“ Extracted: Video Editing/Making\n",
      "  No skills found\n",
      "Processing job 30/40\n",
      "Fetching skills from: https://internshala.com/internship/detail/finance-internship-in-gurgaon-at-ireed-academy-india-private-limited1749537609\n",
      "âœ“ Extracted: Finance\n",
      "  No skills found\n",
      "Processing job 31/40\n",
      "Fetching skills from: https://internshala.com/internship/detail/work-from-home-compute-platform-engineer-internship-at-corteva-agriscience1749479642\n",
      "âœ“ Extracted: Compute Platform Engineer\n",
      "  No skills found\n",
      "Processing job 32/40\n",
      "Fetching skills from: https://internshala.com/internship/detail/corporate-booking-executive-oncall-internship-in-mumbai-at-mahindra-logistics-limited1749476423\n",
      "âœ“ Extracted: Corporate Booking Executive\n",
      "  Skills: Analytics Time Management...\n",
      "Processing job 33/40\n",
      "Fetching skills from: https://internshala.com/internship/detail/hris-intern-internship-in-mumbai-at-mahindra-logistics-limited1749475853\n",
      "âœ“ Extracted: HR Information Systems\n",
      "  Skills: Analytics Team Management VLOOKUP...\n",
      "Processing job 34/40\n",
      "Fetching skills from: https://internshala.com/internship/detail/operations-executive-b2c-internship-in-mumbai-at-mahindra-logistics-limited1749475428\n",
      "âœ“ Extracted: Operations Executive - B2C\n",
      "  No skills found\n",
      "Processing job 35/40\n",
      "Fetching skills from: https://internshala.com/internship/detail/operations-executive-central-initiatives-internship-in-mumbai-at-mahindra-logistics-limited1749474429\n",
      "âœ“ Extracted: Operations Executive - Central Initiatives\n",
      "  Skills: Analytics Stakeholder Management...\n",
      "Processing job 36/40\n",
      "Fetching skills from: https://internshala.com/internship/detail/work-from-home-part-time-sales-and-marketing-internship-at-cook-n-klean1749470335\n",
      "âœ“ Extracted: Sales and Marketing\n",
      "  No skills found\n",
      "Processing job 37/40\n",
      "Fetching skills from: https://internshala.com/internship/detail/inside-sales-internship-in-bangalore-at-unacademy-sorting-hat-technologies-private-limited1749466282\n",
      "âœ“ Extracted: Inside Sales\n",
      "  Skills: Private Limited)...\n",
      "Processing job 38/40\n",
      "Fetching skills from: https://internshala.com/internship/detail/sales-internship-in-multiple-locations-at-aditya-birla-fashion-retail-limited-abfrl1749455195\n",
      "âœ“ Extracted: Sales\n",
      "  Skills: Sales...\n",
      "Processing job 39/40\n",
      "Fetching skills from: https://internshala.com/internship/detail/corporate-sales-internship-in-bangalore-at-reckitt1749450152\n",
      "âœ“ Extracted: Corporate Sales\n",
      "  No skills found\n",
      "Processing job 40/40\n",
      "Fetching skills from: https://internshala.com/internship/detail/mechanical-engineering-internship-in-mumbai-at-theobroma-foods-private-limited1749212973\n",
      "âœ“ Extracted: Mechanical Engineering\n",
      "  Skills: Maintenance...\n",
      "Page 2 complete. Total jobs: 90\n",
      "\n",
      "=== SCRAPING COMPLETE ===\n",
      "Total jobs scraped: 90\n",
      "\n",
      "Data saved to internship_jobs_with_skills.csv\n",
      "Jobs with skills: 41\n",
      "Jobs with URLs: 90\n",
      "\n",
      "=== SUMMARY REPORT ===\n",
      "Total Jobs: 90\n",
      "Jobs with Skills: 41\n",
      "Jobs with URLs: 90\n",
      "Unique Skills Found: 36\n",
      "Top 10 Skills: {'Effective Communication': 3, 'Sales': 2, 'Social Media Marketing': 2, 'Private Limited)': 2, 'Public Speaking': 1, 'MS-Excel Negotiation Problem Solving': 1, 'Financial planning': 1, 'English Proficiency (Spoken)': 1, 'Business Research Data Analysis Lead Generation': 1, 'Effective Communication MS-Office': 1}\n",
      "\n",
      "=== SAMPLE DATA ===\n",
      "\n",
      "Job 1:\n",
      "Title: Public Speaking & Theatre Coach\n",
      "URL: https://internshala.com/internship/detail/part-time-public-speaking-theatre-coach-internship-in-multiple-locations-at-utsaah-learning-private-limited1750914763\n",
      "Skills: Public Speaking\n",
      "\n",
      "Job 2:\n",
      "Title: CA Articleship\n",
      "URL: https://internshala.com/internship/detail/ca-articleship-internship-in-delhi-at-vimal-tandon-co1749710338\n",
      "Skills: \n",
      "\n",
      "Job 3:\n",
      "Title: Campus Ambassador\n",
      "URL: https://internshala.com/internship/detail/campus-ambassador-programme-at-billion-hearts1750314817\n",
      "Skills: \n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import re\n",
    "from datetime import datetime\n",
    "import random\n",
    "from urllib.parse import urljoin, urlparse\n",
    "\n",
    "class JobSkillsExtractor:\n",
    "    def __init__(self):\n",
    "        self.headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
    "            'Accept-Language': 'en-US,en;q=0.5',\n",
    "            'Accept-Encoding': 'gzip, deflate',\n",
    "            'Connection': 'keep-alive',\n",
    "            'Upgrade-Insecure-Requests': '1'\n",
    "        }\n",
    "        self.session = requests.Session()\n",
    "        self.session.headers.update(self.headers)\n",
    "        self.jobs_data = []\n",
    "    \n",
    "    def extract_job_title(self, container):\n",
    "        \"\"\"Extract job title from container\"\"\"\n",
    "        job_title = \"Not specified\"\n",
    "        \n",
    "        # Try multiple selectors for job title\n",
    "        title_selectors = [\n",
    "            'h3.heading_4_5',\n",
    "            'h4.heading_4_5', \n",
    "            'h3',\n",
    "            'h4',\n",
    "            '.job-title',\n",
    "            '.internship-title',\n",
    "            'a.view_detail_button',\n",
    "            'a[href*=\"internship\"]'\n",
    "        ]\n",
    "        \n",
    "        for selector in title_selectors:\n",
    "            title_elem = container.select_one(selector)\n",
    "            if title_elem:\n",
    "                job_title = title_elem.get_text(strip=True)\n",
    "                # Clean up the title\n",
    "                if job_title and len(job_title) < 200:  # Reasonable title length\n",
    "                    break\n",
    "        \n",
    "        # Fallback: search in container text\n",
    "        if job_title == \"Not specified\":\n",
    "            container_text = container.get_text()\n",
    "            lines = container_text.split('\\n')\n",
    "            for line in lines:\n",
    "                line = line.strip()\n",
    "                if 'intern' in line.lower() and len(line) < 100 and len(line) > 5:\n",
    "                    job_title = line\n",
    "                    break\n",
    "        \n",
    "        return job_title\n",
    "    \n",
    "    def extract_job_url(self, container):\n",
    "        \"\"\"Extract the detailed job/internship URL\"\"\"\n",
    "        job_url = None\n",
    "        \n",
    "        # Try multiple selectors for job detail URL\n",
    "        url_selectors = [\n",
    "            'a[href*=\"/internship/detail/\"]',\n",
    "            'a[href*=\"/internship/\"]',\n",
    "            '.view_detail_button a',\n",
    "            '.view-details a',\n",
    "            'a[title*=\"detail\"]',\n",
    "            'h3 a',\n",
    "            'h4 a',\n",
    "            '.heading_4_5 a'\n",
    "        ]\n",
    "        \n",
    "        for selector in url_selectors:\n",
    "            links = container.select(selector)\n",
    "            for link in links:\n",
    "                href = link.get('href', '')\n",
    "                if href and ('internship' in href or 'job' in href):\n",
    "                    # Make absolute URL if relative\n",
    "                    if not href.startswith('http'):\n",
    "                        job_url = 'https://internshala.com' + href\n",
    "                    else:\n",
    "                        job_url = href\n",
    "                    break\n",
    "            if job_url:\n",
    "                break\n",
    "        \n",
    "        return job_url\n",
    "    \n",
    "    def extract_skills_from_listing(self, container):\n",
    "        \"\"\"Extract skills from the job listing container\"\"\"\n",
    "        skills_raw = \"\"\n",
    "        \n",
    "        # Try to find skills in the listing itself\n",
    "        skills_selectors = [\n",
    "            '.round_tabs_container',\n",
    "            '.round_tabs',\n",
    "            '.skills-required',\n",
    "            '.skills',\n",
    "            '.technologies'\n",
    "        ]\n",
    "        \n",
    "        for selector in skills_selectors:\n",
    "            skills_container = container.select_one(selector)\n",
    "            if skills_container:\n",
    "                skills_raw = skills_container.get_text(' ', strip=True)\n",
    "                break\n",
    "            else:\n",
    "                # Try multiple elements\n",
    "                skill_elements = container.select(selector)\n",
    "                if skill_elements:\n",
    "                    skills_raw = ', '.join([skill.get_text(strip=True) for skill in skill_elements])\n",
    "                    break\n",
    "        \n",
    "        # Fallback: search in container text\n",
    "        if not skills_raw:\n",
    "            container_text = container.get_text()\n",
    "            skills_patterns = [\n",
    "                r'Skills?[:\\s]*([^\\n]+)',\n",
    "                r'Requirements?[:\\s]*([^\\n]+)', \n",
    "                r'Tools?[:\\s]*([^\\n]+)',\n",
    "                r'Technologies?[:\\s]*([^\\n]+)',\n",
    "                r'Qualifications?[:\\s]*([^\\n]+)'\n",
    "            ]\n",
    "            \n",
    "            for pattern in skills_patterns:\n",
    "                skills_match = re.search(pattern, container_text, re.IGNORECASE)\n",
    "                if skills_match:\n",
    "                    skills_raw = skills_match.group(1).strip()\n",
    "                    break\n",
    "        \n",
    "        return self.clean_skills(skills_raw)\n",
    "    \n",
    "    def extract_skills_from_detail_page(self, job_url):\n",
    "        \"\"\"Extract detailed skills from the job detail page\"\"\"\n",
    "        if not job_url:\n",
    "            return \"\"\n",
    "        \n",
    "        try:\n",
    "            print(f\"Fetching skills from: {job_url}\")\n",
    "            response = self.session.get(job_url, timeout=15)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            # Multiple selectors to find skills on detail page\n",
    "            detail_skills_selectors = [\n",
    "                '.round_tabs_container',\n",
    "                '.skills-required',\n",
    "                '.skill-tags',\n",
    "                '.requirements',\n",
    "                '.qualifications',\n",
    "                'div[class*=\"skill\"]',\n",
    "                'section[class*=\"skill\"]',\n",
    "                '.round_tabs',\n",
    "                'span.round_tabs',\n",
    "                '.other_details_class .round_tabs_container'\n",
    "            ]\n",
    "            \n",
    "            skills_text = \"\"\n",
    "            \n",
    "            for selector in detail_skills_selectors:\n",
    "                try:\n",
    "                    skills_section = soup.select_one(selector)\n",
    "                    if skills_section:\n",
    "                        skills_text = skills_section.get_text(' ', strip=True)\n",
    "                        if skills_text and len(skills_text) > 3:\n",
    "                            break\n",
    "                    else:\n",
    "                        # Try multiple elements\n",
    "                        skill_elements = soup.select(selector)\n",
    "                        if skill_elements:\n",
    "                            skills_list = []\n",
    "                            for elem in skill_elements:\n",
    "                                text = elem.get_text(strip=True)\n",
    "                                if text and len(text) < 50:  # Individual skill shouldn't be too long\n",
    "                                    skills_list.append(text)\n",
    "                            if skills_list:\n",
    "                                skills_text = ', '.join(skills_list)\n",
    "                                break\n",
    "                except Exception as e:\n",
    "                    continue\n",
    "            \n",
    "            # Fallback: search in page text for skills section\n",
    "            if not skills_text:\n",
    "                page_text = soup.get_text()\n",
    "                skills_patterns = [\n",
    "                    r'Skills?[:\\s]*([^\\n\\r]{10,200})',\n",
    "                    r'Requirements?[:\\s]*([^\\n\\r]{10,200})',\n",
    "                    r'Qualifications?[:\\s]*([^\\n\\r]{10,200})',\n",
    "                    r'Technologies?[:\\s]*([^\\n\\r]{10,200})',\n",
    "                    r'Tools?[:\\s]*([^\\n\\r]{10,200})'\n",
    "                ]\n",
    "                \n",
    "                for pattern in skills_patterns:\n",
    "                    match = re.search(pattern, page_text, re.IGNORECASE | re.MULTILINE)\n",
    "                    if match:\n",
    "                        skills_text = match.group(1).strip()\n",
    "                        break\n",
    "            \n",
    "            return self.clean_skills(skills_text)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching skills from detail page {job_url}: {e}\")\n",
    "            return \"\"\n",
    "    \n",
    "    def clean_skills(self, skills_text):\n",
    "        \"\"\"Clean and standardize skills text\"\"\"\n",
    "        if not skills_text or skills_text == \"Not specified\":\n",
    "            return \"\"\n",
    "        \n",
    "        # Remove unwanted characters but keep common programming symbols\n",
    "        skills_text = re.sub(r'[^\\w\\s,.\\-+#()]', '', skills_text)\n",
    "        \n",
    "        # Skill mappings for standardization\n",
    "        skill_mappings = {\n",
    "            'photoshop': 'Adobe Photoshop',\n",
    "            'illustrator': 'Adobe Illustrator', \n",
    "            'ms office': 'Microsoft Office',\n",
    "            'excel': 'Microsoft Excel',\n",
    "            'powerpoint': 'Microsoft PowerPoint',\n",
    "            'word': 'Microsoft Word',\n",
    "            'js': 'JavaScript',\n",
    "            'html5': 'HTML',\n",
    "            'css3': 'CSS',\n",
    "            'nodejs': 'Node.js',\n",
    "            'reactjs': 'React.js',\n",
    "            'vuejs': 'Vue.js',\n",
    "            'angularjs': 'Angular',\n",
    "            'mysql': 'MySQL',\n",
    "            'postgresql': 'PostgreSQL',\n",
    "            'mongodb': 'MongoDB'\n",
    "        }\n",
    "        \n",
    "        skills = []\n",
    "        # Split by common separators\n",
    "        for separator in [',', 'and', '&', '|', ';']:\n",
    "            if separator in skills_text.lower():\n",
    "                parts = re.split(rf'\\s*{re.escape(separator)}\\s*', skills_text, flags=re.IGNORECASE)\n",
    "                break\n",
    "        else:\n",
    "            parts = [skills_text]\n",
    "        \n",
    "        for part in parts:\n",
    "            part = part.strip()\n",
    "            if part and len(part) > 1:\n",
    "                # Further split by 'and'\n",
    "                subparts = re.split(r'\\s+and\\s+', part, flags=re.IGNORECASE)\n",
    "                for subpart in subparts:\n",
    "                    subpart = subpart.strip()\n",
    "                    if subpart and len(subpart) > 1:\n",
    "                        subpart_lower = subpart.lower()\n",
    "                        mapped_skill = skill_mappings.get(subpart_lower, subpart)\n",
    "                        if len(mapped_skill) > 1 and len(mapped_skill) < 50:  # Reasonable skill length\n",
    "                            skills.append(mapped_skill)\n",
    "        \n",
    "        # Remove duplicates and limit to reasonable number\n",
    "        unique_skills = []\n",
    "        seen = set()\n",
    "        for skill in skills:\n",
    "            skill_lower = skill.lower()\n",
    "            if skill_lower not in seen and len(unique_skills) < 15:\n",
    "                unique_skills.append(skill)\n",
    "                seen.add(skill_lower)\n",
    "        \n",
    "        return ', '.join(unique_skills)\n",
    "    \n",
    "    def scrape_jobs_with_skills(self, max_pages=3, fetch_from_detail=True):\n",
    "        \"\"\"Scrape jobs with title, URL and skills\"\"\"\n",
    "        print(\"Starting job scraping with skills extraction...\")\n",
    "        base_url = \"https://internshala.com/internships\"\n",
    "        \n",
    "        for page in range(1, max_pages + 1):\n",
    "            try:\n",
    "                print(f\"\\nScraping page {page}...\")\n",
    "                url = f\"{base_url}/page-{page}\" if page > 1 else base_url\n",
    "                \n",
    "                response = self.session.get(url, timeout=10)\n",
    "                response.raise_for_status()\n",
    "                \n",
    "                soup = BeautifulSoup(response.content, 'html.parser')\n",
    "                \n",
    "               \n",
    "                internship_containers = []\n",
    "                selectors_to_try = [\n",
    "                    'div.internship_meta',\n",
    "                    'div[class*=\"internship\"]',\n",
    "                    'div.individual_internship', \n",
    "                    'div.internship-item',\n",
    "                    'div[data-internship-id]',\n",
    "                    'div.container-fluid.individual_internship'\n",
    "                ]\n",
    "                \n",
    "                for selector in selectors_to_try:\n",
    "                    try:\n",
    "                        containers = soup.select(selector)\n",
    "                        if containers:\n",
    "                            internship_containers = containers\n",
    "                            print(f\"Found {len(containers)} jobs using selector: {selector}\")\n",
    "                            break\n",
    "                    except:\n",
    "                        continue\n",
    "                \n",
    "                if not internship_containers:\n",
    "                    print(f\"No job containers found on page {page}\")\n",
    "                    continue\n",
    "                \n",
    "                for i, container in enumerate(internship_containers):\n",
    "                    try:\n",
    "                        print(f\"Processing job {i+1}/{len(internship_containers)}\")\n",
    "                        \n",
    "                        job_title = self.extract_job_title(container)\n",
    "                        job_url = self.extract_job_url(container)\n",
    "                        skills_from_listing = self.extract_skills_from_listing(container)\n",
    "                        \n",
    "                        detailed_skills = \"\"\n",
    "                        if fetch_from_detail and job_url:\n",
    "                            detailed_skills = self.extract_skills_from_detail_page(job_url)\n",
    "                            time.sleep(random.uniform(1, 2))  # Be respectful\n",
    "                        \n",
    "                        # Combine skills (prefer detailed skills)\n",
    "                        final_skills = detailed_skills if detailed_skills else skills_from_listing\n",
    "                        \n",
    "                        job_data = {\n",
    "                            'job_title': job_title,\n",
    "                            'job_url': job_url,\n",
    "                            'skills_required': final_skills,\n",
    "                            'skills_from_listing': skills_from_listing,\n",
    "                            'skills_from_detail': detailed_skills,\n",
    "                            'scraped_on': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "                        }\n",
    "                        \n",
    "                        if job_title != \"Not specified\":\n",
    "                            self.jobs_data.append(job_data)\n",
    "                            print(f\"âœ“ Extracted: {job_title}\")\n",
    "                            if final_skills:\n",
    "                                print(f\"  Skills: {final_skills[:100]}...\")\n",
    "                            else:\n",
    "                                print(\"  No skills found\")\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        print(f\"Error processing job {i+1}: {e}\")\n",
    "                        continue\n",
    "                \n",
    "                print(f\"Page {page} complete. Total jobs: {len(self.jobs_data)}\")\n",
    "                time.sleep(random.uniform(2, 4))  # Page delay\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error scraping page {page}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        print(f\"\\n=== SCRAPING COMPLETE ===\")\n",
    "        print(f\"Total jobs scraped: {len(self.jobs_data)}\")\n",
    "    \n",
    "    def save_jobs_data(self, filename='jobs_with_skills.csv'):\n",
    "        \"\"\"Save job data to CSV\"\"\"\n",
    "        if not self.jobs_data:\n",
    "            print(\"No job data to save\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        df = pd.DataFrame(self.jobs_data)\n",
    "        \n",
    "        # Add some analytics\n",
    "        df['has_skills'] = df['skills_required'].apply(lambda x: len(x.strip()) > 0)\n",
    "        df['skills_count'] = df['skills_required'].apply(lambda x: len(x.split(',')) if x else 0)\n",
    "        df['has_url'] = df['job_url'].notna()\n",
    "        \n",
    "        df.to_csv(filename, index=False)\n",
    "        print(f\"\\nData saved to {filename}\")\n",
    "        print(f\"Jobs with skills: {len(df[df['has_skills']])}\")\n",
    "        print(f\"Jobs with URLs: {len(df[df['has_url']])}\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def get_skills_summary(self):\n",
    "        \"\"\"Get summary of extracted skills\"\"\"\n",
    "        if not self.jobs_data:\n",
    "            return \"No data available\"\n",
    "        \n",
    "        df = pd.DataFrame(self.jobs_data)\n",
    "        \n",
    "        # Extract all unique skills\n",
    "        all_skills = []\n",
    "        for skills_str in df['skills_required']:\n",
    "            if skills_str:\n",
    "                skills_list = [skill.strip() for skill in skills_str.split(',')]\n",
    "                all_skills.extend(skills_list)\n",
    "        \n",
    "        # Count skill frequency\n",
    "        from collections import Counter\n",
    "        skill_counts = Counter(all_skills)\n",
    "        \n",
    "        summary = {\n",
    "            'Total Jobs': len(df),\n",
    "            'Jobs with Skills': len(df[df['skills_required'].str.len() > 0]),\n",
    "            'Jobs with URLs': len(df[df['job_url'].notna()]),\n",
    "            'Unique Skills Found': len(skill_counts),\n",
    "            'Top 10 Skills': dict(skill_counts.most_common(10))\n",
    "        }\n",
    "        \n",
    "        return summary\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to run the job skills extractor\"\"\"\n",
    "    extractor = JobSkillsExtractor()\n",
    "    \n",
    "    # Scrape jobs with skills (set fetch_from_detail=False for faster scraping)\n",
    "    extractor.scrape_jobs_with_skills(max_pages=2, fetch_from_detail=True)\n",
    "    \n",
    "    # Save data\n",
    "    df = extractor.save_jobs_data('internship_jobs_with_skills.csv')\n",
    "    \n",
    "    # Print summary\n",
    "    summary = extractor.get_skills_summary()\n",
    "    print(\"\\n=== SUMMARY REPORT ===\")\n",
    "    for key, value in summary.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "    \n",
    "    # Show sample data\n",
    "    if not df.empty:\n",
    "        print(\"\\n=== SAMPLE DATA ===\")\n",
    "        for idx, row in df.head(3).iterrows():\n",
    "            print(f\"\\nJob {idx+1}:\")\n",
    "            print(f\"Title: {row['job_title']}\")\n",
    "            print(f\"URL: {row['job_url']}\")\n",
    "            print(f\"Skills: {row['skills_required']}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    df = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50650fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import re\n",
    "from datetime import datetime\n",
    "import json\n",
    "from urllib.parse import urljoin, urlparse\n",
    "import random\n",
    "\n",
    "class SkillsExtractionScraper:\n",
    "    def __init__(self):\n",
    "        self.headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
    "            'Accept-Language': 'en-US,en;q=0.5',\n",
    "            'Accept-Encoding': 'gzip, deflate',\n",
    "            'Connection': 'keep-alive',\n",
    "            'Upgrade-Insecure-Requests': '1'\n",
    "        }\n",
    "        self.session = requests.Session()\n",
    "        self.session.headers.update(self.headers)\n",
    "        self.internships_data = []\n",
    "        \n",
    "    def extract_skills_comprehensive(self, container, detail_url=None):\n",
    "        \"\"\"Enhanced skills extraction with multiple fallback methods\"\"\"\n",
    "        skills_found = []\n",
    "        \n",
    "        # Method 1: Look for skills in container using multiple selectors\n",
    "        skills_selectors = [\n",
    "            '.round_tabs_container',  # Main skills container\n",
    "            '.round_tabs',            # Individual skill tags\n",
    "            '.skill_required',        # Skills section\n",
    "            '.requirements',          # Requirements section\n",
    "            '.tags',                  # Tags section\n",
    "            '.skills',               # Skills class\n",
    "            '.qualifications',       \n",
    "            '[class*=\"skill\"]',      \n",
    "            '[class*=\"requirement\"]', \n",
    "            '.badge',                \n",
    "            '.tag',                  \n",
    "        ]\n",
    "        \n",
    "        for selector in skills_selectors:\n",
    "            try:\n",
    "                elements = container.select(selector)\n",
    "                for elem in elements:\n",
    "                    text = elem.get_text(strip=True)\n",
    "                    if text and len(text) > 1:\n",
    "                        skills_found.append(text)\n",
    "            except Exception as e:\n",
    "                continue\n",
    "        \n",
    "        # Method 2: Text pattern matching within container\n",
    "        container_text = container.get_text()\n",
    "        skills_patterns = [\n",
    "            r'Skills?\\s*(?:required|needed)?[:\\-\\s]*([^\\n]+)',\n",
    "            r'Requirements?[:\\-\\s]*([^\\n]+)',\n",
    "            r'Tools?[:\\-\\s]*([^\\n]+)',\n",
    "            r'Technologies?[:\\-\\s]*([^\\n]+)',\n",
    "            r'Qualifications?[:\\-\\s]*([^\\n]+)',\n",
    "            r'Must\\s+(?:have|know)[:\\-\\s]*([^\\n]+)',\n",
    "            r'Experience\\s+(?:in|with)[:\\-\\s]*([^\\n]+)',\n",
    "            r'Knowledge\\s+(?:of|in)[:\\-\\s]*([^\\n]+)',\n",
    "            r'Proficiency\\s+(?:in|with)[:\\-\\s]*([^\\n]+)',\n",
    "        ]\n",
    "        \n",
    "        for pattern in skills_patterns:\n",
    "            try:\n",
    "                matches = re.findall(pattern, container_text, re.IGNORECASE | re.MULTILINE)\n",
    "                for match in matches:\n",
    "                    if match.strip():\n",
    "                        skills_found.append(match.strip())\n",
    "            except Exception as e:\n",
    "                continue\n",
    "        \n",
    "        # Method 3: Extract from detail page if available\n",
    "        if detail_url and not skills_found:\n",
    "            detail_skills = self.extract_skills_from_detail_page(detail_url)\n",
    "            if detail_skills:\n",
    "                skills_found.extend(detail_skills)\n",
    "        \n",
    "        # Method 4: Look for common skill keywords in the full text\n",
    "        if not skills_found:\n",
    "            skills_found = self.extract_skills_from_keywords(container_text)\n",
    "        \n",
    "        # Clean and combine all found skills\n",
    "        if skills_found:\n",
    "            combined_skills = ' '.join(skills_found)\n",
    "            return self.clean_and_standardize_skills(combined_skills)\n",
    "        \n",
    "        return \"\"\n",
    "    \n",
    "    def extract_skills_from_detail_page(self, detail_url):\n",
    "        \"\"\"Extract skills from the detailed internship page\"\"\"\n",
    "        try:\n",
    "            if not detail_url.startswith('http'):\n",
    "                detail_url = 'https://internshala.com' + detail_url\n",
    "            \n",
    "            response = self.session.get(detail_url, timeout=10)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            skills_found = []\n",
    "            \n",
    "            # Look for skills in detail page\n",
    "            detail_selectors = [\n",
    "                '.round_tabs_container',\n",
    "                '.round_tabs',\n",
    "                '.skill-tags',\n",
    "                '.requirements-section',\n",
    "                '.job-description',\n",
    "                '.internship-details',\n",
    "                '[class*=\"skill\"]',\n",
    "                '[class*=\"requirement\"]',\n",
    "                '.tags-container'\n",
    "            ]\n",
    "            \n",
    "            for selector in detail_selectors:\n",
    "                try:\n",
    "                    elements = soup.select(selector)\n",
    "                    for elem in elements:\n",
    "                        text = elem.get_text(strip=True)\n",
    "                        if text and len(text) > 1:\n",
    "                            skills_found.append(text)\n",
    "                except:\n",
    "                    continue\n",
    "            \n",
    "            # Also search in the full page text\n",
    "            page_text = soup.get_text()\n",
    "            text_skills = self.extract_skills_from_keywords(page_text)\n",
    "            skills_found.extend(text_skills)\n",
    "            \n",
    "            return skills_found\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching skills from detail page {detail_url}: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def extract_skills_from_keywords(self, text):\n",
    "        \"\"\"Extract skills based on common technology/skill keywords\"\"\"\n",
    "        # Comprehensive list of common skills and technologies\n",
    "        skill_keywords = {\n",
    "            # Programming Languages\n",
    "            'python', 'java', 'javascript', 'js', 'c++', 'c#', 'php', 'ruby', 'go', 'swift', 'kotlin',\n",
    "            'typescript', 'scala', 'r', 'matlab', 'sql', 'html', 'css', 'html5', 'css3',\n",
    "            \n",
    "            # Frameworks & Libraries\n",
    "            'react', 'angular', 'vue', 'node.js', 'nodejs', 'express', 'django', 'flask', 'spring',\n",
    "            'laravel', 'codeigniter', 'bootstrap', 'jquery', 'redux', 'next.js', 'nuxt.js',\n",
    "            \n",
    "            # Databases\n",
    "            'mysql', 'postgresql', 'mongodb', 'sqlite', 'oracle', 'redis', 'firebase',\n",
    "            \n",
    "            # Tools & Platforms\n",
    "            'git', 'github', 'gitlab', 'docker', 'kubernetes', 'aws', 'azure', 'gcp',\n",
    "            'jenkins', 'jira', 'confluence', 'slack', 'trello', 'asana',\n",
    "            \n",
    "            # Design Tools\n",
    "            'photoshop', 'illustrator', 'figma', 'sketch', 'canva', 'indesign', 'after effects',\n",
    "            'premiere pro', 'xd', 'invision', 'zeplin',\n",
    "            \n",
    "            # Marketing & Analytics\n",
    "            'google analytics', 'seo', 'sem', 'google ads', 'facebook ads', 'social media',\n",
    "            'content marketing', 'email marketing', 'mailchimp', 'hubspot',\n",
    "            \n",
    "            # Office & Productivity\n",
    "            'microsoft office', 'excel', 'powerpoint', 'word', 'google sheets', 'google docs',\n",
    "            'ms office', 'outlook', 'teams',\n",
    "            \n",
    "            # Data Science & AI\n",
    "            'machine learning', 'deep learning', 'tensorflow', 'pytorch', 'pandas', 'numpy',\n",
    "            'scikit-learn', 'jupyter', 'tableau', 'power bi', 'data analysis', 'statistics',\n",
    "            \n",
    "            # Mobile Development\n",
    "            'android', 'ios', 'react native', 'flutter', 'xamarin', 'cordova',\n",
    "            \n",
    "            # Other Technical Skills\n",
    "            'api', 'rest api', 'graphql', 'microservices', 'agile', 'scrum', 'devops',\n",
    "            'testing', 'unit testing', 'automation testing', 'selenium'\n",
    "        }\n",
    "        \n",
    "        found_skills = []\n",
    "        text_lower = text.lower()\n",
    "        \n",
    "        for skill in skill_keywords:\n",
    "            # Look for skill as whole word\n",
    "            pattern = r'\\b' + re.escape(skill.lower()) + r'\\b'\n",
    "            if re.search(pattern, text_lower):\n",
    "                found_skills.append(skill)\n",
    "        \n",
    "        return found_skills\n",
    "    \n",
    "    def clean_and_standardize_skills(self, skills_text):\n",
    "        \"\"\"Clean and standardize skills text with better parsing\"\"\"\n",
    "        if not skills_text or skills_text.strip() == \"\":\n",
    "            return \"\"\n",
    "        \n",
    "        # Remove extra whitespace and newlines\n",
    "        skills_text = re.sub(r'\\s+', ' ', skills_text.strip())\n",
    "        \n",
    "        # Remove common non-skill words\n",
    "        noise_words = [\n",
    "            'required', 'preferred', 'must have', 'nice to have', 'experience',\n",
    "            'knowledge', 'proficiency', 'skills', 'tools', 'technologies',\n",
    "            'qualifications', 'requirements', 'abilities', 'competencies',\n",
    "            'familiarity', 'understanding', 'background', 'expertise'\n",
    "        ]\n",
    "        \n",
    "        for word in noise_words:\n",
    "            skills_text = re.sub(r'\\b' + re.escape(word) + r'\\b', '', skills_text, flags=re.IGNORECASE)\n",
    "        \n",
    "        # Split skills by common delimiters\n",
    "        delimiters = [',', 'â€¢', '|', ';', '&', ' and ', ' or ', '\\n', '/', '\\\\']\n",
    "        skills_list = [skills_text]\n",
    "        \n",
    "        for delimiter in delimiters:\n",
    "            new_skills_list = []\n",
    "            for skill_chunk in skills_list:\n",
    "                new_skills_list.extend([s.strip() for s in skill_chunk.split(delimiter) if s.strip()])\n",
    "            skills_list = new_skills_list\n",
    "        \n",
    "        # Clean individual skills\n",
    "        cleaned_skills = []\n",
    "        for skill in skills_list:\n",
    "            skill = skill.strip()\n",
    "            if skill:\n",
    "                # Remove special characters from start/end\n",
    "                skill = re.sub(r'^[^\\w]+|[^\\w]+$', '', skill)\n",
    "                \n",
    "                # Skip if too short or too long\n",
    "                if len(skill) < 2 or len(skill) > 50:\n",
    "                    continue\n",
    "                \n",
    "                # Skip if it's mostly numbers or special characters\n",
    "                if re.match(r'^[\\d\\s\\W]*$', skill):\n",
    "                    continue\n",
    "                \n",
    "                # Standardize common skills\n",
    "                skill_standardized = self.standardize_skill_name(skill)\n",
    "                if skill_standardized:\n",
    "                    cleaned_skills.append(skill_standardized)\n",
    "        \n",
    "        # Remove duplicates while preserving order\n",
    "        final_skills = []\n",
    "        seen = set()\n",
    "        for skill in cleaned_skills:\n",
    "            skill_lower = skill.lower()\n",
    "            if skill_lower not in seen:\n",
    "                seen.add(skill_lower)\n",
    "                final_skills.append(skill)\n",
    "        \n",
    "        # Limit to top 15 skills to avoid clutter\n",
    "        return ', '.join(final_skills[:15])\n",
    "    \n",
    "    def standardize_skill_name(self, skill):\n",
    "        \"\"\"Standardize skill names to consistent format\"\"\"\n",
    "        skill_lower = skill.lower().strip()\n",
    "        \n",
    "        # Skill standardization mappings\n",
    "        standardizations = {\n",
    "            'js': 'JavaScript',\n",
    "            'javascript': 'JavaScript',\n",
    "            'html5': 'HTML',\n",
    "            'html': 'HTML',\n",
    "            'css3': 'CSS',\n",
    "            'css': 'CSS',\n",
    "            'photoshop': 'Adobe Photoshop',\n",
    "            'illustrator': 'Adobe Illustrator',\n",
    "            'ms office': 'Microsoft Office',\n",
    "            'microsoft office': 'Microsoft Office',\n",
    "            'excel': 'Microsoft Excel',\n",
    "            'powerpoint': 'Microsoft PowerPoint',\n",
    "            'word': 'Microsoft Word',\n",
    "            'nodejs': 'Node.js',\n",
    "            'node.js': 'Node.js',\n",
    "            'reactjs': 'React',\n",
    "            'react.js': 'React',\n",
    "            'vue.js': 'Vue.js',\n",
    "            'vuejs': 'Vue.js',\n",
    "            'python': 'Python',\n",
    "            'java': 'Java',\n",
    "            'c++': 'C++',\n",
    "            'c#': 'C#',\n",
    "            'php': 'PHP',\n",
    "            'mysql': 'MySQL',\n",
    "            'postgresql': 'PostgreSQL',\n",
    "            'mongodb': 'MongoDB',\n",
    "            'git': 'Git',\n",
    "            'github': 'GitHub',\n",
    "            'aws': 'AWS',\n",
    "            'google analytics': 'Google Analytics',\n",
    "            'seo': 'SEO',\n",
    "            'social media marketing': 'Social Media Marketing',\n",
    "            'content writing': 'Content Writing',\n",
    "            'digital marketing': 'Digital Marketing',\n",
    "            'machine learning': 'Machine Learning',\n",
    "            'data analysis': 'Data Analysis',\n",
    "            'artificial intelligence': 'Artificial Intelligence',\n",
    "            'ai': 'Artificial Intelligence',\n",
    "            'ml': 'Machine Learning'\n",
    "        }\n",
    "        \n",
    "        # Return standardized version if available, otherwise return title case\n",
    "        if skill_lower in standardizations:\n",
    "            return standardizations[skill_lower]\n",
    "        elif len(skill_lower) > 1:\n",
    "            return skill.title()\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "    def get_internship_detail_url(self, container):\n",
    "        \"\"\"Extract the detail page URL for the internship\"\"\"\n",
    "        try:\n",
    "            # Common selectors for internship detail links\n",
    "            detail_selectors = [\n",
    "                'a[href*=\"/internship/detail/\"]',\n",
    "                'a[href*=\"/internships/detail/\"]',\n",
    "                'a[href*=\"internship\"][href*=\"detail\"]',\n",
    "                '.view_detail_button a',\n",
    "                '.view-details a',\n",
    "                'a[title*=\"detail\"]',\n",
    "                'a[href*=\"detail\"]',\n",
    "                'h3 a',  # Title links often lead to detail page\n",
    "                'h4 a',\n",
    "                '.heading_4_5 a'\n",
    "            ]\n",
    "            \n",
    "            for selector in detail_selectors:\n",
    "                links = container.select(selector)\n",
    "                for link in links:\n",
    "                    href = link.get('href', '')\n",
    "                    if href and ('detail' in href or 'internship' in href):\n",
    "                        return href\n",
    "            \n",
    "            return None\n",
    "            \n",
    "        except Exception as e:\n",
    "            return None\n",
    "    \n",
    "    def scrape_internshala_with_skills_focus(self, max_pages=5):\n",
    "        \"\"\"Scrape internships from Internshala with enhanced skills extraction\"\"\"\n",
    "        print(\"Starting Internshala scraping with focus on skills extraction...\")\n",
    "        base_url = \"https://internshala.com/internships\"\n",
    "        \n",
    "        for page in range(1, max_pages + 1):\n",
    "            try:\n",
    "                print(f\"Scraping page {page}...\")\n",
    "                url = f\"{base_url}/page-{page}\" if page > 1 else base_url\n",
    "                \n",
    "                response = self.session.get(url, timeout=15)\n",
    "                response.raise_for_status()\n",
    "                \n",
    "                soup = BeautifulSoup(response.content, 'html.parser')\n",
    "                \n",
    "                # Try multiple selectors to find internship containers\n",
    "                internship_containers = []\n",
    "                selectors_to_try = [\n",
    "                    'div.internship_meta',\n",
    "                    'div[class*=\"internship\"]',\n",
    "                    'div.individual_internship',\n",
    "                    'div.container-fluid.individual_internship',\n",
    "                    'div[data-internship-id]'\n",
    "                ]\n",
    "                \n",
    "                for selector in selectors_to_try:\n",
    "                    try:\n",
    "                        containers = soup.select(selector)\n",
    "                        if containers:\n",
    "                            internship_containers = containers\n",
    "                            print(f\"Found {len(containers)} internships using selector: {selector}\")\n",
    "                            break\n",
    "                    except:\n",
    "                        continue\n",
    "                \n",
    "                if not internship_containers:\n",
    "                    print(f\"No internships found on page {page}\")\n",
    "                    continue\n",
    "                \n",
    "                for i, container in enumerate(internship_containers):\n",
    "                    try:\n",
    "                        print(f\"Processing internship {i+1}/{len(internship_containers)}\")\n",
    "                        \n",
    "                        # Extract basic info\n",
    "                        job_title = self.extract_job_title(container)\n",
    "                        company_name = self.extract_company_name(container)\n",
    "                        detail_url = self.get_internship_detail_url(container)\n",
    "                        \n",
    "                        # Extract skills with enhanced method\n",
    "                        skills_required = self.extract_skills_comprehensive(container, detail_url)\n",
    "                        \n",
    "                        if job_title != \"Not specified\" and skills_required:\n",
    "                            internship_data = {\n",
    "                                'platform': 'Internshala',\n",
    "                                'company_name': company_name,\n",
    "                                'job_title': job_title,\n",
    "                                'skills_required': skills_required,\n",
    "                                'detail_url': detail_url,\n",
    "                                'scraped_on': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "                            }\n",
    "                            \n",
    "                            self.internships_data.append(internship_data)\n",
    "                            print(f\"âœ“ {job_title} at {company_name}\")\n",
    "                            print(f\"  Skills: {skills_required[:100]}...\")\n",
    "                            \n",
    "                    except Exception as e:\n",
    "                        print(f\"Error processing internship {i+1}: {e}\")\n",
    "                        continue\n",
    "                \n",
    "                print(f\"Page {page} complete. Total internships: {len(self.internships_data)}\")\n",
    "                time.sleep(random.uniform(2, 4))\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error scraping page {page}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        print(f\"Scraping complete! Found {len(self.internships_data)} internships with skills data\")\n",
    "    \n",
    "    def extract_job_title(self, container):\n",
    "        \"\"\"Extract job title from container\"\"\"\n",
    "        title_selectors = [\n",
    "            'h3.heading_4_5 a',\n",
    "            'h4.heading_4_5 a', \n",
    "            'h3 a',\n",
    "            'h4 a',\n",
    "            '.job-title',\n",
    "            '.internship-title',\n",
    "            'h3.heading_4_5',\n",
    "            'h4.heading_4_5'\n",
    "        ]\n",
    "        \n",
    "        for selector in title_selectors:\n",
    "            elem = container.select_one(selector)\n",
    "            if elem:\n",
    "                return elem.get_text(strip=True)\n",
    "        \n",
    "        return \"Not specified\"\n",
    "    \n",
    "    def extract_company_name(self, container):\n",
    "        \"\"\"Extract company name from container\"\"\"\n",
    "        company_selectors = [\n",
    "            'a.link_display_like_text',\n",
    "            'a[href*=\"company\"]',\n",
    "            '.company-name',\n",
    "            'h5 a',\n",
    "            'h4 a'\n",
    "        ]\n",
    "        \n",
    "        for selector in company_selectors:\n",
    "            elem = container.select_one(selector)\n",
    "            if elem:\n",
    "                return elem.get_text(strip=True)\n",
    "        \n",
    "        return \"Not specified\"\n",
    "    \n",
    "    def save_skills_data(self, filename='internship_skills_data.csv'):\n",
    "        \"\"\"Save data with focus on skills to CSV\"\"\"\n",
    "        if not self.internships_data:\n",
    "            print(\"No data to save\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        df = pd.DataFrame(self.internships_data)\n",
    "        \n",
    "        # Add skills analysis\n",
    "        df['skills_count'] = df['skills_required'].apply(lambda x: len(x.split(',')) if x else 0)\n",
    "        df['has_skills'] = df['skills_required'].apply(lambda x: len(x.strip()) > 0 if x else False)\n",
    "        \n",
    "        # Remove duplicates\n",
    "        df = df.drop_duplicates(subset=['company_name', 'job_title'], keep='first')\n",
    "        \n",
    "        # Sort by skills count (most skills first)\n",
    "        df = df.sort_values('skills_count', ascending=False)\n",
    "        \n",
    "        df.to_csv(filename, index=False)\n",
    "        print(f\"Skills data saved to {filename}\")\n",
    "        print(f\"Total internships with skills: {len(df[df['has_skills']])}\")\n",
    "        \n",
    "        return df\n",
    "\n",
    "# Separate focused skills scraper\n",
    "def scrape_skills_only(job_urls_list):\n",
    "    \"\"\"\n",
    "    Separate function to scrape skills from a list of job URLs\n",
    "    \n",
    "    Args:\n",
    "        job_urls_list: List of internship URLs to scrape skills from\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with job URLs and extracted skills\n",
    "    \"\"\"\n",
    "    scraper = SkillsExtractionScraper()\n",
    "    skills_data = []\n",
    "    \n",
    "    for i, url in enumerate(job_urls_list):\n",
    "        try:\n",
    "            print(f\"Processing URL {i+1}/{len(job_urls_list)}: {url[:50]}...\")\n",
    "            \n",
    "            if not url.startswith('http'):\n",
    "                url = 'https://internshala.com' + url\n",
    "            \n",
    "            response = scraper.session.get(url, timeout=15)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            # Extract title\n",
    "            title_elem = soup.select_one('h1, .job-title, .internship-title') or soup.select_one('title')\n",
    "            job_title = title_elem.get_text(strip=True) if title_elem else \"Not specified\"\n",
    "            \n",
    "            # Extract company\n",
    "            company_elem = soup.select_one('.company-name, [class*=\"company\"]')\n",
    "            company_name = company_elem.get_text(strip=True) if company_elem else \"Not specified\"\n",
    "            \n",
    "            # Extract skills using comprehensive method\n",
    "            skills_required = scraper.extract_skills_comprehensive(soup, url)\n",
    "            \n",
    "            skills_data.append({\n",
    "                'job_url': url,\n",
    "                'job_title': job_title,\n",
    "                'company_name': company_name,\n",
    "                'skills_required': skills_required,\n",
    "                'skills_count': len(skills_required.split(',')) if skills_required else 0,\n",
    "                'scraped_on': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "            })\n",
    "            \n",
    "            print(f\"âœ“ Found {len(skills_required.split(',')) if skills_required else 0} skills\")\n",
    "            time.sleep(random.uniform(1, 3))\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {url}: {e}\")\n",
    "            skills_data.append({\n",
    "                'job_url': url,\n",
    "                'job_title': \"Error\",\n",
    "                'company_name': \"Error\", \n",
    "                'skills_required': \"\",\n",
    "                'skills_count': 0,\n",
    "                'error': str(e)\n",
    "            })\n",
    "    \n",
    "    df = pd.DataFrame(skills_data)\n",
    "    df.to_csv('skills_from_urls.csv', index=False)\n",
    "    print(f\"Skills data saved to skills_from_urls.csv\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Usage examples\n",
    "def main():\n",
    "    # Option 1: Enhanced scraping with skills focus\n",
    "    print(\"=== OPTION 1: Enhanced Skills Scraping ===\")\n",
    "    scraper = SkillsExtractionScraper()\n",
    "    scraper.scrape_internshala_with_skills_focus(max_pages=3)\n",
    "    df = scraper.save_skills_data('enhanced_internship_skills.csv')\n",
    "    \n",
    "    # Option 2: Scrape skills from specific URLs\n",
    "    print(\"\\n=== OPTION 2: Skills from Specific URLs ===\")\n",
    "    # Example URLs - replace with your actual URLs\n",
    "    sample_urls = [\n",
    "        \"/internship/detail/web-development-internship-in-bangalore-at-tech-company1234\",\n",
    "        \"/internship/detail/digital-marketing-internship-in-mumbai-at-marketing-firm5678\"\n",
    "    ]\n",
    "    \n",
    "    # Uncomment to use:\n",
    "    # skills_df = scrape_skills_only(sample_urls)\n",
    "    \n",
    "    return df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    df = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2afa6696",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Starting internship scraping...\n",
      "INFO:__main__:Scraping internships from internshala.com\n",
      "INFO:__main__:Searching: site:internshala.com \"web development internship\" internship 2024 2025 -blog -article -news\n",
      "INFO:__main__:Found 0 results for 'web development internship'\n",
      "INFO:__main__:Searching: site:internshala.com \"software development internship\" internship 2024 2025 -blog -article -news\n",
      "INFO:__main__:Found 0 results for 'software development internship'\n",
      "INFO:__main__:Searching: site:internshala.com \"data science internship\" internship 2024 2025 -blog -article -news\n",
      "INFO:__main__:Found 0 results for 'data science internship'\n",
      "INFO:__main__:Searching: site:internshala.com \"digital marketing internship\" internship 2024 2025 -blog -article -news\n",
      "INFO:__main__:Found 0 results for 'digital marketing internship'\n",
      "INFO:__main__:Searching: site:internshala.com \"graphic design internship\" internship 2024 2025 -blog -article -news\n",
      "INFO:__main__:Found 0 results for 'graphic design internship'\n",
      "INFO:__main__:Searching: site:internshala.com \"content writing internship\" internship 2024 2025 -blog -article -news\n",
      "INFO:__main__:Found 0 results for 'content writing internship'\n",
      "INFO:__main__:Searching: site:internshala.com \"python internship\" internship 2024 2025 -blog -article -news\n",
      "INFO:__main__:Found 0 results for 'python internship'\n",
      "INFO:__main__:Searching: site:internshala.com \"react internship\" internship 2024 2025 -blog -article -news\n",
      "INFO:__main__:Found 0 results for 'react internship'\n",
      "INFO:__main__:Searching: site:internshala.com \"frontend internship\" internship 2024 2025 -blog -article -news\n",
      "INFO:__main__:Found 0 results for 'frontend internship'\n",
      "INFO:__main__:Searching: site:internshala.com \"backend internship\" internship 2024 2025 -blog -article -news\n",
      "INFO:__main__:Found 0 results for 'backend internship'\n",
      "INFO:__main__:Searching: site:internshala.com \"ui ux internship\" internship 2024 2025 -blog -article -news\n",
      "INFO:__main__:Found 0 results for 'ui ux internship'\n",
      "INFO:__main__:Searching: site:internshala.com \"mobile app internship\" internship 2024 2025 -blog -article -news\n",
      "INFO:__main__:Found 0 results for 'mobile app internship'\n",
      "INFO:__main__:Searching: site:internshala.com \"business development internship\" internship 2024 2025 -blog -article -news\n",
      "INFO:__main__:Found 0 results for 'business development internship'\n",
      "INFO:__main__:Searching: site:internshala.com \"hr internship\" internship 2024 2025 -blog -article -news\n",
      "INFO:__main__:Found 0 results for 'hr internship'\n",
      "INFO:__main__:Searching: site:internshala.com \"finance internship\" internship 2024 2025 -blog -article -news\n",
      "INFO:__main__:Found 0 results for 'finance internship'\n",
      "INFO:__main__:Searching: site:internshala.com \"internship 2024\" internship 2024 2025 -blog -article -news\n",
      "INFO:__main__:Found 0 results for 'internship 2024'\n",
      "INFO:__main__:Searching: site:internshala.com \"internship 2025\" internship 2024 2025 -blog -article -news\n",
      "INFO:__main__:Found 0 results for 'internship 2025'\n",
      "INFO:__main__:Searching: site:internshala.com \"summer internship\" internship 2024 2025 -blog -article -news\n",
      "INFO:__main__:Found 0 results for 'summer internship'\n",
      "INFO:__main__:Searching: site:internshala.com \"winter internship\" internship 2024 2025 -blog -article -news\n",
      "INFO:__main__:Found 0 results for 'winter internship'\n",
      "INFO:__main__:Total unique results found: 0\n",
      "INFO:__main__:Collected 0 internships from internshala.com\n",
      "INFO:__main__:Scraping internships from hellointern.com\n",
      "INFO:__main__:Searching: site:hellointern.com \"web development internship\" internship 2024 2025 -blog -article -news\n",
      "INFO:__main__:Found 0 results for 'web development internship'\n",
      "INFO:__main__:Searching: site:hellointern.com \"software development internship\" internship 2024 2025 -blog -article -news\n",
      "INFO:__main__:Found 0 results for 'software development internship'\n",
      "INFO:__main__:Searching: site:hellointern.com \"data science internship\" internship 2024 2025 -blog -article -news\n",
      "INFO:__main__:Found 0 results for 'data science internship'\n",
      "INFO:__main__:Searching: site:hellointern.com \"digital marketing internship\" internship 2024 2025 -blog -article -news\n",
      "INFO:__main__:Found 0 results for 'digital marketing internship'\n",
      "INFO:__main__:Searching: site:hellointern.com \"graphic design internship\" internship 2024 2025 -blog -article -news\n",
      "INFO:__main__:Found 0 results for 'graphic design internship'\n",
      "INFO:__main__:Searching: site:hellointern.com \"content writing internship\" internship 2024 2025 -blog -article -news\n",
      "INFO:__main__:Found 0 results for 'content writing internship'\n",
      "INFO:__main__:Searching: site:hellointern.com \"python internship\" internship 2024 2025 -blog -article -news\n",
      "INFO:__main__:Found 0 results for 'python internship'\n",
      "INFO:__main__:Searching: site:hellointern.com \"react internship\" internship 2024 2025 -blog -article -news\n",
      "INFO:__main__:Found 0 results for 'react internship'\n",
      "INFO:__main__:Searching: site:hellointern.com \"frontend internship\" internship 2024 2025 -blog -article -news\n",
      "INFO:__main__:Found 0 results for 'frontend internship'\n",
      "INFO:__main__:Searching: site:hellointern.com \"backend internship\" internship 2024 2025 -blog -article -news\n",
      "INFO:__main__:Found 0 results for 'backend internship'\n",
      "INFO:__main__:Searching: site:hellointern.com \"ui ux internship\" internship 2024 2025 -blog -article -news\n",
      "INFO:__main__:Found 0 results for 'ui ux internship'\n",
      "INFO:__main__:Searching: site:hellointern.com \"mobile app internship\" internship 2024 2025 -blog -article -news\n",
      "INFO:__main__:Found 0 results for 'mobile app internship'\n",
      "INFO:__main__:Searching: site:hellointern.com \"business development internship\" internship 2024 2025 -blog -article -news\n",
      "INFO:__main__:Found 0 results for 'business development internship'\n",
      "INFO:__main__:Searching: site:hellointern.com \"hr internship\" internship 2024 2025 -blog -article -news\n",
      "INFO:__main__:Found 0 results for 'hr internship'\n",
      "INFO:__main__:Searching: site:hellointern.com \"finance internship\" internship 2024 2025 -blog -article -news\n",
      "INFO:__main__:Found 0 results for 'finance internship'\n",
      "INFO:__main__:Searching: site:hellointern.com \"internship 2024\" internship 2024 2025 -blog -article -news\n",
      "INFO:__main__:Found 0 results for 'internship 2024'\n",
      "INFO:__main__:Searching: site:hellointern.com \"internship 2025\" internship 2024 2025 -blog -article -news\n",
      "INFO:__main__:Found 0 results for 'internship 2025'\n",
      "INFO:__main__:Searching: site:hellointern.com \"summer internship\" internship 2024 2025 -blog -article -news\n",
      "INFO:__main__:Found 0 results for 'summer internship'\n",
      "INFO:__main__:Searching: site:hellointern.com \"winter internship\" internship 2024 2025 -blog -article -news\n",
      "INFO:__main__:Found 0 results for 'winter internship'\n",
      "INFO:__main__:Total unique results found: 0\n",
      "INFO:__main__:Collected 0 internships from hellointern.com\n",
      "INFO:__main__:Scraping internships from linkedin.com\n",
      "INFO:__main__:Searching: site:linkedin.com \"web development internship\" internship 2024 2025 -blog -article -news\n",
      "INFO:__main__:Found 0 results for 'web development internship'\n",
      "INFO:__main__:Searching: site:linkedin.com \"software development internship\" internship 2024 2025 -blog -article -news\n",
      "INFO:__main__:Found 0 results for 'software development internship'\n",
      "INFO:__main__:Searching: site:linkedin.com \"data science internship\" internship 2024 2025 -blog -article -news\n",
      "INFO:__main__:Found 0 results for 'data science internship'\n",
      "INFO:__main__:Searching: site:linkedin.com \"digital marketing internship\" internship 2024 2025 -blog -article -news\n",
      "INFO:__main__:Found 0 results for 'digital marketing internship'\n",
      "INFO:__main__:Searching: site:linkedin.com \"graphic design internship\" internship 2024 2025 -blog -article -news\n",
      "INFO:__main__:Found 0 results for 'graphic design internship'\n",
      "INFO:__main__:Searching: site:linkedin.com \"content writing internship\" internship 2024 2025 -blog -article -news\n",
      "INFO:__main__:Found 0 results for 'content writing internship'\n",
      "INFO:__main__:Searching: site:linkedin.com \"python internship\" internship 2024 2025 -blog -article -news\n",
      "INFO:__main__:Found 0 results for 'python internship'\n",
      "INFO:__main__:Searching: site:linkedin.com \"react internship\" internship 2024 2025 -blog -article -news\n",
      "INFO:__main__:Found 0 results for 'react internship'\n",
      "INFO:__main__:Searching: site:linkedin.com \"frontend internship\" internship 2024 2025 -blog -article -news\n",
      "INFO:__main__:Found 0 results for 'frontend internship'\n",
      "INFO:__main__:Searching: site:linkedin.com \"backend internship\" internship 2024 2025 -blog -article -news\n",
      "INFO:__main__:Found 0 results for 'backend internship'\n",
      "INFO:__main__:Searching: site:linkedin.com \"ui ux internship\" internship 2024 2025 -blog -article -news\n",
      "INFO:__main__:Found 0 results for 'ui ux internship'\n",
      "INFO:__main__:Searching: site:linkedin.com \"mobile app internship\" internship 2024 2025 -blog -article -news\n",
      "INFO:__main__:Found 0 results for 'mobile app internship'\n",
      "INFO:__main__:Searching: site:linkedin.com \"business development internship\" internship 2024 2025 -blog -article -news\n",
      "INFO:__main__:Found 0 results for 'business development internship'\n",
      "INFO:__main__:Searching: site:linkedin.com \"hr internship\" internship 2024 2025 -blog -article -news\n",
      "INFO:__main__:Found 0 results for 'hr internship'\n",
      "INFO:__main__:Searching: site:linkedin.com \"finance internship\" internship 2024 2025 -blog -article -news\n",
      "INFO:__main__:Found 0 results for 'finance internship'\n",
      "INFO:__main__:Searching: site:linkedin.com \"internship 2024\" internship 2024 2025 -blog -article -news\n",
      "INFO:__main__:Found 0 results for 'internship 2024'\n",
      "INFO:__main__:Searching: site:linkedin.com \"internship 2025\" internship 2024 2025 -blog -article -news\n",
      "INFO:__main__:Found 0 results for 'internship 2025'\n",
      "INFO:__main__:Searching: site:linkedin.com \"summer internship\" internship 2024 2025 -blog -article -news\n",
      "INFO:__main__:Found 0 results for 'summer internship'\n",
      "INFO:__main__:Searching: site:linkedin.com \"winter internship\" internship 2024 2025 -blog -article -news\n",
      "INFO:__main__:Found 0 results for 'winter internship'\n",
      "INFO:__main__:Total unique results found: 0\n",
      "INFO:__main__:Collected 0 internships from linkedin.com\n",
      "INFO:__main__:Total unique internships collected: 0\n",
      "WARNING:__main__:No data to save. Run scrape_internships() first.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== DETAILED STATISTICS ===\n",
      "\"No data available. Run scrape_internships() first.\"\n",
      "\n",
      "Recent internships (last 30 days): 0\n",
      "Scraping failed. Please check your SerpAPI key and internet connection.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c11d9b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
